[
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html",
    "title": "Modeling fish (Complete)",
    "section": "",
    "text": "Practice modeling using the fish dataset on two common fish species in fish market sales.\n\nWe will use the tidyverse package for data wrangling and visualization and the tidymodels package for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nThese data come from Kaggle and is commonly used in machine learning examples.\n\nfish &lt;- read_csv(\"https://data-science-with-r.github.io/data/fish.csv\")\n\nThe data dictionary is below:\n\n\nvariable\ndescription\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\nLet’s take a look at the data.\n\nfish\n\n# A tibble: 55 × 7\n   species weight length_vertical length_diagonal length_cross height width\n   &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bream      242            23.2            25.4         30     11.5  4.02\n 2 Bream      290            24              26.3         31.2   12.5  4.31\n 3 Bream      340            23.9            26.5         31.1   12.4  4.70\n 4 Bream      363            26.3            29           33.5   12.7  4.46\n 5 Bream      430            26.5            29           34     12.4  5.13\n 6 Bream      450            26.8            29.7         34.7   13.6  4.93\n 7 Bream      500            26.8            29.7         34.5   14.2  5.28\n 8 Bream      390            27.6            30           35     12.7  4.69\n 9 Bream      450            27.6            30           35.1   14.0  4.84\n10 Bream      500            28.5            30.7         36.2   14.2  4.96\n# ℹ 45 more rows"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#goal",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#goal",
    "title": "Modeling fish (Complete)",
    "section": "",
    "text": "Practice modeling using the fish dataset on two common fish species in fish market sales."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#packages",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#packages",
    "title": "Modeling fish (Complete)",
    "section": "",
    "text": "We will use the tidyverse package for data wrangling and visualization and the tidymodels package for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#data",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#data",
    "title": "Modeling fish (Complete)",
    "section": "",
    "text": "These data come from Kaggle and is commonly used in machine learning examples.\n\nfish &lt;- read_csv(\"https://data-science-with-r.github.io/data/fish.csv\")\n\nThe data dictionary is below:\n\n\nvariable\ndescription\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\nLet’s take a look at the data.\n\nfish\n\n# A tibble: 55 × 7\n   species weight length_vertical length_diagonal length_cross height width\n   &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bream      242            23.2            25.4         30     11.5  4.02\n 2 Bream      290            24              26.3         31.2   12.5  4.31\n 3 Bream      340            23.9            26.5         31.1   12.4  4.70\n 4 Bream      363            26.3            29           33.5   12.7  4.46\n 5 Bream      430            26.5            29           34     12.4  5.13\n 6 Bream      450            26.8            29.7         34.7   13.6  4.93\n 7 Bream      500            26.8            29.7         34.5   14.2  5.28\n 8 Bream      390            27.6            30           35     12.7  4.69\n 9 Bream      450            27.6            30           35.1   14.0  4.84\n10 Bream      500            28.5            30.7         36.2   14.2  4.96\n# ℹ 45 more rows"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#visualizing-the-model",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#visualizing-the-model",
    "title": "Modeling fish (Complete)",
    "section": "Visualizing the model",
    "text": "Visualizing the model\nWe’re going to investigate the relationship between the weights and heights of fish, predicting weight from height.\n\nCreate an appropriate plot to investigate this relationship. Add appropriate labels to the plot.\n\n\nggplot(fish, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n\n\n\n\n\n\n\nIf you were to draw a a straight line to best represent the relationship between the heights and weights of fish, where would it go? Why?\n\nStart from the bottom and go up. Identify the first and last point and draw a line through most the others.\n\nNow, let R draw the line for you! Hint: Use geom_smooth().\n\n\nggplot(fish, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Weights vs. lengths of fish\",\n    x = \"Head-to-tail lentgh (cm)\",\n    y = \"Weight of fish (grams)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhat types of questions can this plot help answer?\n\nIs there a relationship between fish heights and weights of fish?\n\nWe can use this line to make predictions. Predict what you think the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm. Which prediction is considered extrapolation?\n\nAt 10 cm, we estimate a weight of 375 grams. At 15 cm, we estimate a weight of 600 grams At 20 cm, we estimate a weight of 975 grams. 20 cm would be considered extrapolation.\n\nWhat is a residual?\n\nDifference between predicted and observed."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#model-fitting",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#model-fitting",
    "title": "Modeling fish (Complete)",
    "section": "Model fitting",
    "text": "Model fitting\n\nFit a model to predict fish weights from their heights.\n\n\nfish_hw_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ height, data = fish)\n\nfish_hw_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = weight ~ height, data = data)\n\nCoefficients:\n(Intercept)       height  \n    -288.42        60.92  \n\n\n\nPredict what the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm using this model.\n\n\nx &lt;- c(10, 15, 20)\n-288 + 60.92 * x\n\n[1] 321.2 625.8 930.4\n\n\n\nCalculate predicted weights for all fish in the data and visualize the residuals under this model.\n\n\nfish_hw_aug &lt;- augment(fish_hw_fit, new_data = fish)\n\nggplot(fish_hw_aug, aes(x = height, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +  \n  geom_segment(aes(xend = height, yend = .pred), color = \"gray\") +  \n  geom_point(aes(y = .pred), shape = \"circle open\") + \n  theme_minimal() +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    subtitle = \"Residuals\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#model-summary",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#model-summary",
    "title": "Modeling fish (Complete)",
    "section": "Model summary",
    "text": "Model summary\n\nDisplay the model summary including estimates for the slope and intercept along with measurements of uncertainty around them. Show how you can extract these values from the model output.\n\n\nfish_hw_tidy &lt;- tidy(fish_hw_fit)\nfish_hw_tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -288.      34.0      -8.49 1.83e-11\n2 height          60.9      2.64     23.1  2.40e-29\n\n\n\nWrite out your model using mathematical notation.\n\n\\(\\widehat{weight} = -288 + 60.9 \\times height\\)"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#correlation",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#correlation",
    "title": "Modeling fish (Complete)",
    "section": "Correlation",
    "text": "Correlation\nWe can also assess correlation between two quantitative variables.\n\nWhat is correlation? What are values correlation can take?\n\nStrength and direction of a linear relationship. It’s bounded by -1 and 1.\n\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\nWhat is the correlation between heights and weights of fish?\n\n\nfish |&gt;\n  summarize(r = cor(height, weight))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.954"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#adding-a-third-variable",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#adding-a-third-variable",
    "title": "Modeling fish (Complete)",
    "section": "Adding a third variable",
    "text": "Adding a third variable\n\nDoes the relationship between heights and weights of fish change if we take into consideration species? Plot two separate straight lines for the Bream and Roach species.\n\n\nggplot(fish, aes(x = height, y = weight, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#fitting-other-models",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#fitting-other-models",
    "title": "Modeling fish (Complete)",
    "section": "Fitting other models",
    "text": "Fitting other models\n\nWe can fit more models than just a straight line. Use method = \"loess\". What is different from the plot created before?\n\n\nggplot(fish, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html",
    "title": "Modeling loan interest rates (Complete)",
    "section": "",
    "text": "Practice modeling with multiple predictors using the data on loan interest rates.\n\nThe dataset is about loans from the peer-to-peer lender, Lending Club, from the openintro package. We will use tidyverse and tidymodels for data exploration and modeling, respectively.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\n\n\nBefore we use the dataset, we’ll make a few transformations to it.\n\nReview the code below with your neighbor and write a summary of the data transformation pipeline.\n\nAdd response here.\n\nloans &lt;- loans_full_schema |&gt;\n  mutate(\n    credit_util = total_credit_utilized / total_credit_limit,\n    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),\n    verified_income = droplevels(verified_income),\n    homeownership = str_to_title(homeownership),\n    homeownership = fct_relevel(homeownership, \"Rent\", \"Mortgage\", \"Own\")\n  ) |&gt;\n  rename(credit_checks = inquiries_last_12m) |&gt;\n  select(\n    interest_rate, loan_amount, verified_income, \n    debt_to_income, credit_util, bankruptcy, term, \n    credit_checks, issue_month, homeownership\n  )\n\nHere is a glimpse at the data:\n\nglimpse(loans)\n\nRows: 10,000\nColumns: 10\n$ interest_rate   &lt;dbl&gt; 14.07, 12.61, 17.09, 6.72, 14.07, 6.72, 13.59, 11.99, …\n$ loan_amount     &lt;int&gt; 28000, 5000, 2000, 21600, 23000, 5000, 24000, 20000, 2…\n$ verified_income &lt;fct&gt; Verified, Not Verified, Source Verified, Not Verified,…\n$ debt_to_income  &lt;dbl&gt; 18.01, 5.04, 21.15, 10.16, 57.96, 6.46, 23.66, 16.19, …\n$ credit_util     &lt;dbl&gt; 0.54759517, 0.15003472, 0.66134832, 0.19673228, 0.7549…\n$ bankruptcy      &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ term            &lt;dbl&gt; 60, 36, 36, 36, 36, 36, 60, 60, 36, 36, 60, 60, 36, 60…\n$ credit_checks   &lt;int&gt; 6, 1, 4, 0, 7, 6, 1, 1, 3, 0, 4, 4, 8, 6, 0, 0, 4, 6, …\n$ issue_month     &lt;fct&gt; Mar-2018, Feb-2018, Feb-2018, Jan-2018, Mar-2018, Jan-…\n$ homeownership   &lt;fct&gt; Mortgage, Rent, Rent, Rent, Rent, Own, Mortgage, Mortg…"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#goal",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#goal",
    "title": "Modeling loan interest rates (Complete)",
    "section": "",
    "text": "Practice modeling with multiple predictors using the data on loan interest rates."
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#packages",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#packages",
    "title": "Modeling loan interest rates (Complete)",
    "section": "",
    "text": "The dataset is about loans from the peer-to-peer lender, Lending Club, from the openintro package. We will use tidyverse and tidymodels for data exploration and modeling, respectively.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#data-prep",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#data-prep",
    "title": "Modeling loan interest rates (Complete)",
    "section": "",
    "text": "Before we use the dataset, we’ll make a few transformations to it.\n\nReview the code below with your neighbor and write a summary of the data transformation pipeline.\n\nAdd response here.\n\nloans &lt;- loans_full_schema |&gt;\n  mutate(\n    credit_util = total_credit_utilized / total_credit_limit,\n    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),\n    verified_income = droplevels(verified_income),\n    homeownership = str_to_title(homeownership),\n    homeownership = fct_relevel(homeownership, \"Rent\", \"Mortgage\", \"Own\")\n  ) |&gt;\n  rename(credit_checks = inquiries_last_12m) |&gt;\n  select(\n    interest_rate, loan_amount, verified_income, \n    debt_to_income, credit_util, bankruptcy, term, \n    credit_checks, issue_month, homeownership\n  )\n\nHere is a glimpse at the data:\n\nglimpse(loans)\n\nRows: 10,000\nColumns: 10\n$ interest_rate   &lt;dbl&gt; 14.07, 12.61, 17.09, 6.72, 14.07, 6.72, 13.59, 11.99, …\n$ loan_amount     &lt;int&gt; 28000, 5000, 2000, 21600, 23000, 5000, 24000, 20000, 2…\n$ verified_income &lt;fct&gt; Verified, Not Verified, Source Verified, Not Verified,…\n$ debt_to_income  &lt;dbl&gt; 18.01, 5.04, 21.15, 10.16, 57.96, 6.46, 23.66, 16.19, …\n$ credit_util     &lt;dbl&gt; 0.54759517, 0.15003472, 0.66134832, 0.19673228, 0.7549…\n$ bankruptcy      &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ term            &lt;dbl&gt; 60, 36, 36, 36, 36, 36, 60, 60, 36, 36, 60, 60, 36, 60…\n$ credit_checks   &lt;int&gt; 6, 1, 4, 0, 7, 6, 1, 1, 3, 0, 4, 4, 8, 6, 0, 0, 4, 6, …\n$ issue_month     &lt;fct&gt; Mar-2018, Feb-2018, Feb-2018, Jan-2018, Mar-2018, Jan-…\n$ homeownership   &lt;fct&gt; Mortgage, Rent, Rent, Rent, Rent, Own, Mortgage, Mortg…"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#get-to-know-the-data",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#get-to-know-the-data",
    "title": "Modeling loan interest rates (Complete)",
    "section": "Get to know the data",
    "text": "Get to know the data\n\nWhat is a typical interest rate in this dataset? What are some attributes of a typical loan and a typical borrower. Give yourself no more than 5 minutes for this exploration and share 1-2 findings.\n\nggplot(loans, aes(x = interest_rate)) +\n  geom_histogram(binwidth = 1)\nggplot(loans, aes(x = loan_amount)) +\n  geom_histogram(binwidth = 5000)\nggplot(loans, aes(x = term)) +\n  geom_bar()\nggplot(loans, aes(x = issue_month)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(loans, aes(x = credit_util)) +\n  geom_histogram(binwidth = 0.1)\nggplot(loans, aes(x = verified_income)) +\n  geom_bar()\nggplot(loans, aes(x = debt_to_income)) +\n  geom_histogram(binwidth = 10)\nggplot(loans, aes(x = bankruptcy)) +\n  geom_bar()\nggplot(loans, aes(x = credit_checks)) +\n  geom_bar()\nggplot(loans, aes(x = homeownership)) +\n  geom_bar()"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#interest-rate-vs.-credit-utilization",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#interest-rate-vs.-credit-utilization",
    "title": "Modeling loan interest rates (Complete)",
    "section": "Interest rate vs. credit utilization",
    "text": "Interest rate vs. credit utilization\n\nFor a regression model for predicting interest rate from credit utilization. Display the summary output.\n\n\nrate_util_fit &lt;- linear_reg() |&gt;\n  fit(interest_rate ~ credit_util, data = loans)\n\ntidy(rate_util_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    10.5     0.0871     121.  0        \n2 credit_util     4.73    0.180       26.3 1.18e-147\n\n\n\nVisualize the model.\n\n\nggplot(loans, aes(x = credit_util, y = interest_rate)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nInterpret the intercept and the slope.\n\nIntercept: Borrowers with 0 credit utilization are predicted, on average, to get an interest rate of 10.5%.\nSlope: For each additional point credit utilization is higher, interest rate is predicted to be higher, on average, by 4.73%."
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#interest-rate-vs.-homeownership",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#interest-rate-vs.-homeownership",
    "title": "Modeling loan interest rates (Complete)",
    "section": "Interest rate vs. homeownership",
    "text": "Interest rate vs. homeownership\n\nFit a regression model for predicting interest rate from homeownership and display the summary output.\n\n\nrate_home_fit &lt;- linear_reg() |&gt;\n  fit(interest_rate ~ homeownership, data = loans)\n\ntidy(rate_home_fit)\n\n# A tibble: 3 × 5\n  term                  estimate std.error statistic  p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)             12.9      0.0803    161.   0       \n2 homeownershipMortgage   -0.866    0.108      -8.03 1.08e-15\n3 homeownershipOwn        -0.611    0.158      -3.88 1.06e- 4\n\n\n\n\nInterpret each coefficient in context of the problem.\n\nIntercept: Loan applicants who rent are predicted to receive an interest rate of 12.9%, on average.\n\nSlopes:\n\nThe model predicts that loan applicants who have a mortgage for their home receive 0.866% lower interest rate than those who rent their home, on average.\nThe model predicts that loan applicants who own their home receive 0.611% lower interest rate than those who rent their home, on average."
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#interest-rate-vs.-credit-utilization-and-homeownership",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#interest-rate-vs.-credit-utilization-and-homeownership",
    "title": "Modeling loan interest rates (Complete)",
    "section": "Interest rate vs. credit utilization and homeownership",
    "text": "Interest rate vs. credit utilization and homeownership\nMain effects model\n\nFit a regression model to predict interest rate from credit utilization and homeownership, without an interaction effect between the two predictors. Display the summary output.\n\n\nrate_util_home_fit &lt;- linear_reg() |&gt;\n  fit(interest_rate ~ credit_util + homeownership, data = loans)\n\ntidy(rate_util_home_fit)\n\n# A tibble: 4 × 5\n  term                  estimate std.error statistic   p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)              9.93      0.140    70.8   0        \n2 credit_util              5.34      0.207    25.7   2.20e-141\n3 homeownershipMortgage    0.696     0.121     5.76  8.71e-  9\n4 homeownershipOwn         0.128     0.155     0.827 4.08e-  1\n\n\n\nWrite the estimated regression equation for loan applications from each of the homeownership groups separately.\n\nRent: \\(\\widehat{interest~rate} = 9.93 + 5.34 \\times credit~util\\)\n\nMortgage: \\(\\widehat{interest~rate} = 10.626 + 5.34 \\times credit~util\\)\n\nOwn: \\(\\widehat{interest~rate} = 10.058 + 5.34 \\times credit~util\\)\n\n\n\nHow does the model predict the interest rate to vary as credit utilization varies for loan applicants with different homeownership status. Are the rates the same or different?\n\nThe same.\nInteraction effects model\n\nFit a regression model to predict interest rate from credit utilization and homeownership, with an interaction effect between the two predictors. Display the summary output.\n\n\nrate_util_home_int_fit &lt;- linear_reg() |&gt;\n  fit(interest_rate ~ credit_util * homeownership, data = loans)\n\ntidy(rate_util_home_int_fit)\n\n# A tibble: 6 × 5\n  term                              estimate std.error statistic  p.value\n  &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                          9.44      0.199     47.5  0       \n2 credit_util                          6.20      0.325     19.1  1.01e-79\n3 homeownershipMortgage                1.39      0.228      6.11 1.04e- 9\n4 homeownershipOwn                     0.697     0.316      2.20 2.75e- 2\n5 credit_util:homeownershipMortgage   -1.64      0.457     -3.58 3.49e- 4\n6 credit_util:homeownershipOwn        -1.06      0.590     -1.80 7.24e- 2\n\n\n\nWrite the estimated regression equation for loan applications from each of the homeownership groups separately.\n\nRent: \\(\\widehat{interest~rate} = 9.44 + 6.20 \\times credit~util\\)\n\nMortgage: \\(\\widehat{interest~rate} = 10.83 + 4.56 \\times credit~util\\)\n\nOwn: \\(\\widehat{interest~rate} = 10.137 + 5.14 \\times credit~util\\)\n\n\n\nHow does the model predict the interest rate to vary as credit utilization varies for loan applicants with different homeownership status. Are the rates the same or different?\n\nDifferent.\nChoosing a model\nRule of thumb: Occam’s Razor - Don’t over-complicate the situation! We prefer the simplest best model.\n\nDisplay model level summary statistics.\n\n\nglance(rate_util_home_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0682        0.0679  4.83      244. 1.25e-152     3 -29926. 59861. 59897.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(rate_util_home_int_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0694        0.0689  4.83      149. 4.79e-153     5 -29919. 59852. 59903.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nWhat is R-squared? What is adjusted R-squared?\n\nR-squared is the percent variability in the response that is explained by our model. (Can use when models have same number of variables for model selection)\nAdjusted R-squared is similar, but has a penalty for the number of variables in the model. (Should use for model selection when models have different numbers of variables).\n\nBased on the adjusted \\(R^2\\)s of these two models, which one do we prefer?\n\nThe interaction effects model, though just barely."
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#another-model-to-consider",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates-complete.html#another-model-to-consider",
    "title": "Modeling loan interest rates (Complete)",
    "section": "Another model to consider",
    "text": "Another model to consider\n\nLet’s add one more model to the variable – issue month. Should we add this variable to the interaction effects model from earlier?\n\n\nlinear_reg() |&gt;\n  fit(interest_rate ~ credit_util * homeownership + issue_month, data = loans) |&gt;\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0694        0.0688  4.83      106. 5.62e-151     7 -29919. 59856. 59921.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNo, the adjusted R-squared goes down."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n Modeling and inference",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nLanguage of models\n\n\n\n\n\nLinear regression with a numerical predictor\n\n\n\n\n\nLinear regression with a categorical predictor\n\n\n\n\n\nOutliers in linear regression\n\n\n\n\n\nLinear regression with multiple predictors\n\n\n\n\n\nMain and interaction effects\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "\n Modeling and inference",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nLanguage of models\n\n\n\n\n\nLinear regression with a numerical predictor\n\n\n\n\n\nLinear regression with a categorical predictor\n\n\n\n\n\nOutliers in linear regression\n\n\n\n\n\nLinear regression with multiple predictors\n\n\n\n\n\nMain and interaction effects\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#code-alongs",
    "href": "index.html#code-alongs",
    "title": "\n Modeling and inference",
    "section": "Code alongs",
    "text": "Code alongs\n\n\n\n\nTitle\n\n\n\n\n\n\nModeling fish (Complete)\n\n\n\n\n\nModeling fish\n\n\n\n\n\nModeling loan interest rates (Complete)\n\n\n\n\n\nModeling loan interest rates\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "\n Modeling and inference",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nTitle\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling",
    "title": "Language of models",
    "section": "Modeling",
    "text": "Modeling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but remember there are many many other types of models too!)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-cars",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-cars",
    "title": "Language of models",
    "section": "Modeling cars",
    "text": "Modeling cars\n\n\nWhat is the relationship between cars’ weights and their mileage?\nWhat is your best guess for a car’s MPG that weighs 3,500 pounds?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars",
    "title": "Language of models",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nDescribe: What is the relationship between cars’ weights and their mileage?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars-1",
    "title": "Language of models",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-2",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-2",
    "title": "Language of models",
    "section": "Modeling",
    "text": "Modeling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but there are many many other types of models too!)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-vocabulary",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-vocabulary",
    "title": "Language of models",
    "section": "Modeling vocabulary",
    "text": "Modeling vocabulary\n\nOutcome: Variable whose behavior or variation you are trying to understand, on the y-axis (aka response variable, dependent variable)\nPredictor(s): Other variable(s) that you want to use to explain the variation in the outcome, on the x-axis (aka explanatory variable(s), independent variable(s))\nModel function: The regression line for predicting the outcome variable from the predictor variable(s), comprised generally of an intercept and a slope for each predictor\nPredicted value: Output of the model function, which gives the typical (expected) value of the outcome conditioning on the predictor\n\nResiduals: A measure of how far each case’s observed value is from its predicted value (based on a particular model)\n\nResidual = Observed value - Predicted value\nTells how far above/below the expected value each case is"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#predictor",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#predictor",
    "title": "Language of models",
    "section": "Predictor",
    "text": "Predictor\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#outcome",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#outcome",
    "title": "Language of models",
    "section": "Outcome",
    "text": "Outcome\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line",
    "title": "Language of models",
    "section": "Regression line",
    "text": "Regression line"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-slope",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-slope",
    "title": "Language of models",
    "section": "Regression line: slope",
    "text": "Regression line: slope"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-intercept",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-intercept",
    "title": "Language of models",
    "section": "Regression line: intercept",
    "text": "Regression line: intercept"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation",
    "title": "Language of models",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation-1",
    "title": "Language of models",
    "section": "Correlation",
    "text": "Correlation\n\nRanges between -1 and 1.\nSame sign as the slope."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#visualizing-the-model",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#visualizing-the-model",
    "title": "Language of models",
    "section": "Visualizing the model",
    "text": "Visualizing the model\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#residuals",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#residuals",
    "title": "Language of models",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#extending-regression-lines",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#extending-regression-lines",
    "title": "Language of models",
    "section": "Extending regression lines",
    "text": "Extending regression lines"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#models---upsides-and-downsides",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#models---upsides-and-downsides",
    "title": "Language of models",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modeling over simple visual inspection of data.\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#variation-around-the-model",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#variation-around-the-model",
    "title": "Language of models",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\nStatistics is the explanation of variation in the context of what remains unexplained.\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#how-do-we-use-models",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#how-do-we-use-models",
    "title": "Language of models",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nPredict / classify: Plug in the value(s) of predictor(s) to the model to obtain the predicted value of the outcome\nDescribe: Quantify the relationship between predictor(s) and outcome with slopes"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#predict-classify-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#predict-classify-1",
    "title": "Language of models",
    "section": "Predict / classify",
    "text": "Predict / classify\n\nHow do self-driving cars decide whether an object in front of them is a human, another car, or a trash can?\nHow does an online shopping website decide which ad to serve to you for the next item you might purchase?\nWhat happens if either of these get it wrong?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp",
    "title": "Language of models",
    "section": "Leisure, commute, physical activity and BP",
    "text": "Leisure, commute, physical activity and BP\n\nRelation Between Leisure Time, Commuting, and Occupational Physical Activity With Blood Pressure in 125,402 Adults: The Lifelines Cohort\nByambasukh, Oyuntugs, Harold Snieder, and Eva Corpeleijn. “Relation between leisure time, commuting, and occupational physical activity with blood pressure in 125 402 adults: the lifelines cohort.” Journal of the American Heart Association 9.4 (2020): e014313."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp-1",
    "title": "Language of models",
    "section": "Leisure, commute, physical activity and BP",
    "text": "Leisure, commute, physical activity and BP\nBackground: Whether all domains of daily‐life moderate‐to‐vigorous physical activity (MVPA) are associated with lower blood pressure (BP) and how this association depends on age and body mass index remains unclear.\nMethods and Results: In the population‐based Lifelines cohort (N=125,402), MVPA was assessed by the Short Questionnaire to Assess Health‐Enhancing Physical Activity, a validated questionnaire in different domains such as commuting, leisure‐time, and occupational PA. BP was assessed using the last 3 of 10 measurements after 10 minutes’ rest in the supine position. Hypertension was defined as systolic BP ≥140 mm Hg and/or diastolic BP ≥90 mm Hg and/or use of antihypertensives. In regression analysis, higher commuting and leisure‐time but not occupational MVPA related to lower BP and lower hypertension risk. Commuting‐and‐leisure‐time MVPA was associated with BP in a dose‐dependent manner. β Coefficients (95% CI) from linear regression analyses were −1.64 (−2.03 to −1.24), −2.29 (−2.68 to −1.90), and finally −2.90 (−3.29 to −2.50) mm Hg systolic BP for the low, middle, and highest tertile of MVPA compared with “No MVPA” as the reference group after adjusting for age, sex, education, smoking and alcohol use. Further adjustment for body mass index attenuated the associations by 30% to 50%, but more MVPA remained significantly associated with lower BP and lower risk of hypertension. This association was age dependent. β Coefficients (95% CI) for the highest tertiles of commuting‐and‐leisure‐time MVPA were −1.67 (−2.20 to −1.15), −3.39 (−3.94 to −2.82) and −4.64 (−6.15 to −3.14) mm Hg systolic BP in adults &lt;40, 40 to 60, and &gt;60 years, respectively.\nConclusions: Higher commuting and leisure‐time but not occupational MVPA were significantly associated with lower BP and lower hypertension risk at all ages, but these associations were stronger in older adults."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-in-regression",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-in-regression",
    "title": "Outliers in linear regression",
    "section": "Outliers in regression",
    "text": "Outliers in regression\n\nOutliers are observations that fall far from the main cloud of points.\n\nThey can be outlying in:\n\nthe \\(x\\) direction,\nthe \\(y\\) direction, or\nboth.\n\n\nHowever, being outlying in a univariate sense does not always mean being outlying from the bivariate model.\nPoints that are in-line with the bivariate model usually do not influence the least squares line, even if they are extreme in \\(x\\), \\(y\\), or both."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-and-influence",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-and-influence",
    "title": "Outliers in linear regression",
    "section": "Outliers and influence",
    "text": "Outliers and influence\n\n\n\n\n\n\n\n\n\n\n\n\n\nA: One outlier in the \\(y\\) direction, also outlying in the bivariate model; slightly influences the regression line.\n\nB: One outlier on the right (outlying in \\(x\\) and \\(y\\), but not outlying in the bivariate model); close to the regression line and not influential.\n\nC: One point far from the cloud (outlying in \\(x\\), \\(y\\), and bivariate model); pulls the regression line upward, worsening fit for the main data."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-and-influence-1",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-and-influence-1",
    "title": "Outliers in linear regression",
    "section": "Outliers and influence",
    "text": "Outliers and influence\n\n\n\n\n\n\n\n\n\n\n\n\n\nD: A secondary small cloud of four points (outlying in \\(x\\) and bivariate model); strongly influences the regression line, creating poor fit.\n\nE: Outlier far right (outlying in \\(x\\) and \\(y\\)); the regression line is largely controlled by this single point, imposing a trend where there is none.\n\nF: One outlier far away (outlying in \\(x\\) and \\(y\\)), but in-line with the model; has little influence."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#types-of-outliers",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#types-of-outliers",
    "title": "Outliers in linear regression",
    "section": "Types of outliers",
    "text": "Types of outliers\n\nOutliers: Points or groups of points that stand out from the rest of the data.\nLeverage points: Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with high leverage or leverage points.\n\nInfluential points: Outliers, generally high leverage points, that actually alter the slope or position of the regression line.\n\nWe say a point is influential if omitting it would substantially change the regression model."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#practical-advice",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#practical-advice",
    "title": "Outliers in linear regression",
    "section": "Practical advice",
    "text": "Practical advice\n\nTest your analysis with and without outliers.\nCompare and discuss the impact of outliers on model fit.\nPresent both models to stakeholders to choose the most reasonable interpretation.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nRemoving outliers should only be done with strong justification – excluding interesting or extreme cases can lead to misleading models, poor predictive performance, and flawed conclusions."
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#packages",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#packages",
    "title": "Linear regression with a numerical predictor",
    "section": "Packages",
    "text": "Packages\n\n\nfivethirtyeight for data\n\ntidyverse for data wrangling and visualization\n\ntidymodels for modeling\n\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-prep",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-prep",
    "title": "Linear regression with a numerical predictor",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\n\nRename the dataset as movie_scores\n\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-overview",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-overview",
    "title": "Linear regression with a numerical predictor",
    "section": "Data overview",
    "text": "Data overview\n\nmovie_scores |&gt;\n  select(critics, audience)\n\n# A tibble: 146 × 2\n   critics audience\n     &lt;int&gt;    &lt;int&gt;\n 1      74       86\n 2      85       80\n 3      80       90\n 4      18       84\n 5      14       28\n 6      63       62\n 7      42       53\n 8      86       64\n 9      99       82\n10      89       87\n# ℹ 136 more rows"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-visualization",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-visualization",
    "title": "Linear regression with a numerical predictor",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#regression-model-1",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#regression-model-1",
    "title": "Linear regression with a numerical predictor",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the outcome, \\(Y\\), and the predictor, \\(X\\).\n\\[\n\\begin{aligned}\nY &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#regression-model-2",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#regression-model-2",
    "title": "Linear regression with a numerical predictor",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned}\nY &= \\color{#325b74}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{#325b74}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{#325b74}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#simple-linear-regression",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#simple-linear-regression",
    "title": "Linear regression with a numerical predictor",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nUse simple linear regression to model the relationship between a numerical outcome (\\(Y\\)) and a single numerical predictor (\\(X\\)): \\[\\Large{Y = \\beta_0 + \\beta_1 X + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(\\epsilon\\): Error (residual)"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#simple-linear-regression-1",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#simple-linear-regression-1",
    "title": "Linear regression with a numerical predictor",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\\[\n\\Large{\\hat{Y} = b_0 + b_1 X}\n\\]\n\n\n\\(b_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(b_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\n\nNo error term!"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#choosing-values-for-b_1-and-b_0",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#choosing-values-for-b_1-and-b_0",
    "title": "Linear regression with a numerical predictor",
    "section": "Choosing values for \\(b_1\\) and \\(b_0\\)\n",
    "text": "Choosing values for \\(b_1\\) and \\(b_0\\)"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#residuals",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#residuals",
    "title": "Linear regression with a numerical predictor",
    "section": "Residuals",
    "text": "Residuals\n\n\n\n\n\n\n\n\n\\[\n\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}\n\\]"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#least-squares-line",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#least-squares-line",
    "title": "Linear regression with a numerical predictor",
    "section": "Least squares line",
    "text": "Least squares line\n\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[\ne_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\n\\]\n\n\n\nThe sum of squared residuals is\n\n\\[\ne^2_1 + e^2_2 + \\dots + e^2_n\n\\]\n\n\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#fitting-a-model",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#fitting-a-model",
    "title": "Linear regression with a numerical predictor",
    "section": "Fitting a model",
    "text": "Fitting a model\n\nmovies_fit &lt;- linear_reg() |&gt;\n  fit(audience ~ critics, data = movie_scores)\n\ntidy(movies_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-the-slope",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-the-slope",
    "title": "Linear regression with a numerical predictor",
    "section": "Interpreting the slope",
    "text": "Interpreting the slope\n\nThe slope of the model for predicting audience score from critics score is 0.519. Which of the following is the best interpretation of this value?\n\n\n\n\nFor every one point increase in the critics score, the audience score goes up by 0.519 points, on average.\nFor every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\nFor every one point increase in the critics score, the audience score goes up by 0.519 points.\nFor every one point increase in the audience score, the critics score goes up by 0.519 points, on average."
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-the-slope-1",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-the-slope-1",
    "title": "Linear regression with a numerical predictor",
    "section": "Interpreting the slope",
    "text": "Interpreting the slope\n\nThe slope of the model for predicting audience score from critics score is 0.519. Which of the following is the best interpretation of this value?\n\n\n\n\nFor every one point increase in the critics score, the audience score goes up by 0.519 points, on average.\nFor every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\nFor every one point increase in the critics score, the audience score goes up by 0.519 points.\nFor every one point increase in the audience score, the critics score goes up by 0.519 points, on average."
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-slope-intercept",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-slope-intercept",
    "title": "Linear regression with a numerical predictor",
    "section": "Interpreting slope & intercept",
    "text": "Interpreting slope & intercept\n\\[\n\\widehat{\\text{audience}} = 32.3 + 0.519 \\times \\text{critics}\n\\]\n\n\nSlope: For every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\n\nIntercept: For movies with a critics score of 0, we expect the audience score to be 32.3 points, on average."
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#is-the-intercept-meaningful",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#is-the-intercept-meaningful",
    "title": "Linear regression with a numerical predictor",
    "section": "Is the intercept meaningful?",
    "text": "Is the intercept meaningful?\n✅ The intercept is meaningful in context of the data if\n\nthe predictor can feasibly take values equal to or near zero or\nthe predictor has values near zero in the observed data\n\n\n🛑 Otherwise, it might not be meaningful!"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#least-squares-regression",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#least-squares-regression",
    "title": "Linear regression with a numerical predictor",
    "section": "Least squares regression",
    "text": "Least squares regression\nThe least squares regression line minimizes the sum of squares residuals. It has the following properties:\n\nGoes through the center of mass point (the coordinates corresponding to average \\(X\\) and average \\(Y\\)): \\(b_0 = \\bar{Y} - b_1~\\bar{X}\\)\nSlope of the line has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_Y}{s_X}\\)\nSum of the residuals is zero: \\(\\sum_{i = 1}^n \\epsilon_i = 0\\)\nResiduals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#packages",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#packages",
    "title": "Linear regression with multiple predictors",
    "section": "Packages",
    "text": "Packages\n\n\nDAAG for data\n\ntidyverse for data wrangling and visualization\n\ntidymodels for modeling\n\n\nlibrary(DAAG)\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#data-book-weight-and-volume",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#data-book-weight-and-volume",
    "title": "Linear regression with multiple predictors",
    "section": "Data: Book weight and volume",
    "text": "Data: Book weight and volume\nThe allbacks data frame gives measurements on the volume and weight of 15 books, some of which are paperback and some of which are hardback\n\n\n- volume - cubic centimetres\n- area - square centimetres\n- weight - grams\n- cover - hb or pb\n\n\n\n# A tibble: 15 × 4\n   volume  area weight cover\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n 1    885   382    800 hb   \n 2   1016   468    950 hb   \n 3   1125   387   1050 hb   \n 4    239   371    350 hb   \n 5    701   371    750 hb   \n 6    641   367    600 hb   \n 7   1228   396   1075 hb   \n 8    412     0    250 pb   \n 9    953     0    700 pb   \n10    929     0    650 pb   \n11   1492     0    975 pb   \n12    419     0    350 pb   \n13   1010     0    950 pb   \n14    595     0    425 pb   \n15   1034     0    725 pb   \n\n\n\n\n\nThese books are from the bookshelf of J. H. Maindonald at Australian National University."
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#book-weight-vs.-volume",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#book-weight-vs.-volume",
    "title": "Linear regression with multiple predictors",
    "section": "Book weight vs. volume",
    "text": "Book weight vs. volume\n\n\n\nallbacks_1_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume, data = allbacks)\n\ntidy(allbacks_1_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)  108.      88.4         1.22 0.245     \n2 volume         0.709    0.0975      7.27 0.00000626"
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#book-weight-vs.-volume-and-cover",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#book-weight-vs.-volume-and-cover",
    "title": "Linear regression with multiple predictors",
    "section": "Book weight vs. volume and cover",
    "text": "Book weight vs. volume and cover\n\n\n\nallbacks_2_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume + cover, data = allbacks)\n\ntidy(allbacks_2_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672"
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#interpretation-of-estimates",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#interpretation-of-estimates",
    "title": "Linear regression with multiple predictors",
    "section": "Interpretation of estimates",
    "text": "Interpretation of estimates\n\ntidy(allbacks_2_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n\n\n\n\nSlope - volume: Keeping cover constant, for each additional cubic centimetre books are larger in volume, the model predicts the weight to be higher, on average, by 0.718 grams.\nSlope - cover: Keeping volume constant, the model predicts that paperback books weigh, on average, by 184 grams less than hardback books.\nIntercept: The model predicts that hardback books with 0 volume are expected to weigh 198 grams, on average. (Doesn’t make sense in context.)"
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#r2",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#r2",
    "title": "Linear regression with multiple predictors",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\\(R^2\\) is the percentage of variability in the outcome explained by the regression model.\n\n\nModel 1: weight ~ volume\n\n\n\nglance(allbacks_1_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1     0.803         0.787  124.      52.9 0.00000626     1\n# ℹ 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;,\n#   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nModel 2: weight ~ volume + cover\n\n\n\nglance(allbacks_2_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1     0.927         0.915  78.2      76.7 0.000000145     2\n# ℹ 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;,\n#   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\n\n\\(R^2\\) increases when any predictor is added to the model."
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#adjusted-r2",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#adjusted-r2",
    "title": "Linear regression with multiple predictors",
    "section": "Adjusted \\(R^2\\)\n",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) adds a penalty to \\(R^2\\) for additional predictors in the model, and is therefore a (more) objective measure for comparing models with different numbers of predictors.\n\n\nModel 1: weight ~ volume\n\n\n\nglance(allbacks_1_fit)$adj.r.squared\n\n[1] 0.7874526\n\n\n\n\n\nModel 2: weight ~ volume + cover\n\n\n\nglance(allbacks_2_fit)$adj.r.squared\n\n[1] 0.9153905\n\n\n\n\n\nAdjusted \\(R^2\\) is higher for the model with volume and cover as predictors, and it is therefore the preferable model for predicting weight."
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#model-1---visualized",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#model-1---visualized",
    "title": "Linear regression with multiple predictors",
    "section": "Model 1 - visualized",
    "text": "Model 1 - visualized\n\nglance(allbacks_1_fit) |&gt;\n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.803         0.787"
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#model-2---visualized",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#model-2---visualized",
    "title": "Linear regression with multiple predictors",
    "section": "Model 2 - visualized",
    "text": "Model 2 - visualized\n\nglance(allbacks_2_fit) |&gt;\n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.927         0.915"
  },
  {
    "objectID": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#takeaways",
    "href": "slides/2-1-linear-regression-multiple-predictors/2-1-linear-regression-multiple-predictors.html#takeaways",
    "title": "Linear regression with multiple predictors",
    "section": "Takeaways",
    "text": "Takeaways\n\nWhen interpreting slope coefficients for multiple regression models we need to state that one predictor is kept constant while the other increases.\nAdjusted R-squared is useful when comparing models with different numbers of predictors - it helps you balance model complexity with explanatory power."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#packages",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#packages",
    "title": "Linear regression with a categorical predictor",
    "section": "Packages",
    "text": "Packages\n\n\npalmerpenguins for data\n\ntidyverse for data wrangling and visualization\n\ntidymodels for modeling\n\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#data-penguins",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#data-penguins",
    "title": "Linear regression with a categorical predictor",
    "section": "Data: penguins\n",
    "text": "Data: penguins\n\nWe’ll work with the penguins dataset from the palmerpenguins package, which contains information on body measurements of three species of Antarctic penguins:\n\npenguins\n\n# A tibble: 344 × 7\n   species island    bill_length_mm bill_depth_mm\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7\n 2 Adelie  Torgersen           39.5          17.4\n 3 Adelie  Torgersen           40.3          18  \n 4 Adelie  Torgersen           NA            NA  \n 5 Adelie  Torgersen           36.7          19.3\n 6 Adelie  Torgersen           39.3          20.6\n 7 Adelie  Torgersen           38.9          17.8\n 8 Adelie  Torgersen           39.2          19.6\n 9 Adelie  Torgersen           34.1          18.1\n10 Adelie  Torgersen           42            20.2\n# ℹ 334 more rows\n# ℹ 3 more variables: flipper_length_mm &lt;int&gt;,\n#   body_mass_g &lt;int&gt;, sex &lt;fct&gt;"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#variables",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#variables",
    "title": "Linear regression with a categorical predictor",
    "section": "Variables",
    "text": "Variables\n\nA researcher wants to study the relationship between body weights of penguins based on the island they were recorded on. How are the variables involved in this analysis different?\n\n\n\nOutcome: body weight (numerical)\nPredictor: island (categorical)"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#body-weight-vs.-island",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#body-weight-vs.-island",
    "title": "Linear regression with a categorical predictor",
    "section": "Body weight vs. island",
    "text": "Body weight vs. island\n\nDetermine whether each of the following plot types would be an appropriate choice for visualizing the relationship between body weight and island of penguins.\n\n\nScatterplot ❌\nBox plot ✅\nViolin plot ✅\nDensity plot ✅\nBar plot ❌\nStacked bar plot ❌"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#body-weight-vs.-island-1",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#body-weight-vs.-island-1",
    "title": "Linear regression with a categorical predictor",
    "section": "Body weight vs. island",
    "text": "Body weight vs. island\n\nggplot(\n  penguins, \n  aes(x = body_mass_g, y = island, color = island)\n  ) +\n  geom_boxplot(show.legend = FALSE)\n\nWarning: Removed 2 rows containing non-finite outside the scale\nrange (`stat_boxplot()`)."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#fitting-the-model",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#fitting-the-model",
    "title": "Linear regression with a categorical predictor",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nFit:\n\n\nbm_island_fit &lt;- linear_reg() |&gt;\n  fit(body_mass_g ~ island, data = penguins)\n\n\n\nTidy:\n\n\ntidy(bm_island_fit)\n\n# A tibble: 3 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        4716.      48.5      97.3 8.93e-250\n2 islandDream       -1003.      74.2     -13.5 1.42e- 33\n3 islandTorgersen   -1010.     100.      -10.1 4.66e- 21"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#inspecting-the-model-output",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#inspecting-the-model-output",
    "title": "Linear regression with a categorical predictor",
    "section": "Inspecting the model output",
    "text": "Inspecting the model output\n\nWhy is Biscoe not on the output?\n\n\n\n# A tibble: 3 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        4716.      48.5      97.3 8.93e-250\n2 islandDream       -1003.      74.2     -13.5 1.42e- 33\n3 islandTorgersen   -1010.     100.      -10.1 4.66e- 21\n\n\n\n\nWhen fitting a model with a categorical predictor, the levels of the categorical predictor are encoded to dummy variables, except for one of the levels, the baseline level.\nIn this case Biscoe is the is the baseline level.\nEach slope coefficient describes the predicted difference between heights in that particular school compared to the baseline level."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#dummy-variables",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#dummy-variables",
    "title": "Linear regression with a categorical predictor",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nisland\n\nDummy variable\n\n\n\nDream\nTorgersen\n\n\n\n\nBiscoe\n0\n0\n\n\nDream\n1\n0\n\n\ntorgersen\n0\n1\n\n\n\n\n\n\n\nFor a categorical predictor with \\(k\\) levels, we only need \\(k - 1\\) dummy variables to describe all of its levels:\n\n\nDream = 1 and Torgersen = 0, the penguin is from Dream island.\n\nDream = 0 and Torgersen = 1, the penguin is from Torgersen island.\n\nDream = 0 and Torgersen = 0, the penguin is from Biscoe island, we don’t need a third dummy variable to identify these penguins."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#dummy-coding",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#dummy-coding",
    "title": "Linear regression with a categorical predictor",
    "section": "Dummy coding",
    "text": "Dummy coding\n\n\n\n\n\n\n\n\nNote\n\n\nYou do not need to do anything (i.e., write code) to do the “dummy coding”, R does this under the hood for you when you have a predictor that is categorical (a character or a factor)."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#interpreting-the-model-output",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#interpreting-the-model-output",
    "title": "Linear regression with a categorical predictor",
    "section": "Interpreting the model output",
    "text": "Interpreting the model output\n\\[\n\\widehat{body~mass} = 4716 - 1003 \\times islandDream - 1010 \\times islandTorgersen\n\\]\n\nIntercept: Penguins from Biscoe island are expected to weigh, on average, 4,716 grams.\nSlope - islandDream: Penguins from Dream island are expected to weigh, on average, 1,003 grams less than those from Biscoe island.\nSlope - islandTorgersen: Penguins from Torgersen island are expected to weigh, on average, 1,010 grams less than those from Biscoe island."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#predicting-based-on-the-model",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#predicting-based-on-the-model",
    "title": "Linear regression with a categorical predictor",
    "section": "Predicting based on the model",
    "text": "Predicting based on the model\n\\[\n\\widehat{body~mass} = 4716 - 1003 \\times islandDream - 1010 \\times islandTorgersen\n\\]\n\n\nBiscoe: \\(\\widehat{body~mass} = 4716 - 1003 \\times 0 - 1010 \\times 0 = 4716\\)\n\n\n\n\n\nDream: \\(\\widehat{body~mass} = 4716 - 1003 \\times 1 - 1010 \\times 0 = 3713\\)\n\n\n\n\n\nTorgersen: \\(\\widehat{body~mass} = 4716 - 1003 \\times 0 - 1010 \\times 1 = 3706\\)"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#predicting-based-on-the-model---again",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#predicting-based-on-the-model---again",
    "title": "Linear regression with a categorical predictor",
    "section": "Predicting based on the model - again",
    "text": "Predicting based on the model - again\n\nthree_penguins &lt;- tibble(\n  island = c(\"Biscoe\", \"Dream\", \"Torgersen\")\n  )\n\naugment(bm_island_fit, new_data = three_penguins)\n\n# A tibble: 3 × 2\n  .pred island   \n  &lt;dbl&gt; &lt;chr&gt;    \n1 4716. Biscoe   \n2 3713. Dream    \n3 3706. Torgersen"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#models-with-categorical-predictors",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#models-with-categorical-predictors",
    "title": "Linear regression with a categorical predictor",
    "section": "Models with categorical predictors",
    "text": "Models with categorical predictors\n\nWhen the categorical predictor has many levels, they’re encoded to dummy variables.\nThe first level of the categorical variable is the baseline level. In a model with one categorical predictor, the intercept is the predicted value of the outcome for the baseline level (x = 0).\nEach slope coefficient describes the difference between the predicted value of the outcome for that level of the categorical variable compared to the baseline level."
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#packages",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#packages",
    "title": "Main and interaction effects",
    "section": "Packages",
    "text": "Packages\n\n\nDAAG for data\n\ntidyverse for data wrangling and visualization\n\ntidymodels for modeling\n\n\nlibrary(DAAG)\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#data-book-weight-and-volume",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#data-book-weight-and-volume",
    "title": "Main and interaction effects",
    "section": "Data: Book weight and volume",
    "text": "Data: Book weight and volume\nThe allbacks data frame gives measurements on the volume and weights of 15 books, some of which are paperback and some of which are hardback\n\n\n\nvolume - cubic centimetres\narea - square centimetres\nweight - grams\ncover - hb or pb\n\n\n\n\n# A tibble: 15 × 4\n   volume  area weight cover\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n 1    885   382    800 hb   \n 2   1016   468    950 hb   \n 3   1125   387   1050 hb   \n 4    239   371    350 hb   \n 5    701   371    750 hb   \n 6    641   367    600 hb   \n 7   1228   396   1075 hb   \n 8    412     0    250 pb   \n 9    953     0    700 pb   \n10    929     0    650 pb   \n11   1492     0    975 pb   \n12    419     0    350 pb   \n13   1010     0    950 pb   \n14    595     0    425 pb   \n15   1034     0    725 pb   \n\n\n\n\n\nThese books are from the bookshelf of J. H. Maindonald at Australian National University."
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#two-possible-explanations",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#two-possible-explanations",
    "title": "Main and interaction effects",
    "section": "Two possible explanations",
    "text": "Two possible explanations\n\n\n\nSuppose we want to predict weights of books from their volume and cover type (hardback vs. paperback). Do these visualizations suggest that a model that doesn’t allow for the rate of change in weight to vary by cover type (parallel lines) is a better fit or a model that does allow (non-parallel lines)?"
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#in-pursuit-of-occams-razor",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#in-pursuit-of-occams-razor",
    "title": "Main and interaction effects",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another predictor to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model."
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#in-pursuit-of-occams-razor-1",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#in-pursuit-of-occams-razor-1",
    "title": "Main and interaction effects",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\n\n\nVisually, which of the two models is preferable under Occam’s razor?"
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#model-1-volume-cover",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#model-1-volume-cover",
    "title": "Main and interaction effects",
    "section": "Model 1: volume + cover\n",
    "text": "Model 1: volume + cover\n\nMain effects: Rate of change for weight as volume increases is the same for hardback and paperback books.\n\nallbacks_main_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume + cover, data = allbacks)\n\ntidy(allbacks_main_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n\n\n\n\nglance(allbacks_main_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1     0.927         0.915  78.2      76.7 0.000000145     2\n# ℹ 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;,\n#   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#model-2-volume-cover-volumecover",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#model-2-volume-cover-volumecover",
    "title": "Main and interaction effects",
    "section": "Model 2: volume + cover + volume*cover\n",
    "text": "Model 2: volume + cover + volume*cover\n\nInteraction effects: Rate of change for weight as volume increases is different for hardback and paperback books.\n\nallbacks_int_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume + cover + volume*cover, data = allbacks)\n\ntidy(allbacks_int_fit)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic    p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)     162.       86.5        1.87  0.0887    \n2 volume            0.762     0.0972     7.84  0.00000794\n3 coverpb        -120.      116.        -1.04  0.321     \n4 volume:coverpb   -0.0757    0.128     -0.592 0.566     \n\n\n\n\nglance(allbacks_main_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1     0.927         0.915  78.2      76.7 0.000000145     2\n# ℹ 6 more variables: logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt;,\n#   deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#rs-got-your-back",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#rs-got-your-back",
    "title": "Main and interaction effects",
    "section": "R’s got your back!",
    "text": "R’s got your back!\nWhen you add an interaction effect to a model, R will always add the main effects of those variables too, even if you leave them out of your model formula:\n\nlinear_reg() |&gt;\n  fit(weight ~ volume*cover, data = allbacks) |&gt;\n  tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic    p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)     162.       86.5        1.87  0.0887    \n2 volume            0.762     0.0972     7.84  0.00000794\n3 coverpb        -120.      116.        -1.04  0.321     \n4 volume:coverpb   -0.0757    0.128     -0.592 0.566"
  },
  {
    "objectID": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#choosing-between-models",
    "href": "slides/2-2-main-interaction-effects/2-2-main-interaction-effects.html#choosing-between-models",
    "title": "Main and interaction effects",
    "section": "Choosing between models",
    "text": "Choosing between models\nThe model with the interaction effects has more predictors, and remember that when comparing models with different numbers of predictors, we use adjusted \\(R^2\\) for model selection:\n\n\n\\(R^2\\):\n\nglance(allbacks_main_fit)$r.squared\n\n[1] 0.9274776\n\nglance(allbacks_int_fit)$r.squared\n\n[1] 0.9297137\n\n\n\nAdjusted \\(R^2\\):\n\nglance(allbacks_main_fit)$adj.r.squared\n\n[1] 0.9153905\n\nglance(allbacks_int_fit)$adj.r.squared\n\n[1] 0.9105447\n\n\n\n\n\n\\(R^2\\) is higher for the model with the interaction effect.\nAdjusted \\(R^2\\) is not higher for the model with the interaction effect, therefore we do not need the interaction effect, the main effects model is good enough!"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#transform---visualize",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#transform---visualize",
    "title": "Welcome",
    "section": "Transform <-> visualize",
    "text": "Transform &lt;-&gt; visualize"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#import---tidy",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#import---tidy",
    "title": "Welcome",
    "section": "Import <-> tidy",
    "text": "Import &lt;-&gt; tidy"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#data-science-ethics",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#data-science-ethics",
    "title": "Welcome",
    "section": "Data science ethics",
    "text": "Data science ethics\n\nMisrepresentation\nData privacy\nAlgorithmic bias"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#modeling-and-inference",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#modeling-and-inference",
    "title": "Welcome",
    "section": "Modeling and inference",
    "text": "Modeling and inference\n\n\n\n\n\n\n\n\n\nFitting, interpreting, selecting, and evaluating models\nPrediction, classification, and assessing accuracy\nMaking inferences and quantifying uncertainty"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html",
    "title": "Modeling loan interest rates",
    "section": "",
    "text": "Practice modeling with multiple predictors using the data on loan interest rates.\n\nThe dataset is about loans from the peer-to-peer lender, Lending Club, from the openintro package. We will use tidyverse and tidymodels for data exploration and modeling, respectively.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\n\n\nBefore we use the dataset, we’ll make a few transformations to it.\n\nReview the code below with your neighbor and write a summary of the data transformation pipeline.\n\nAdd response here.\n\nloans &lt;- loans_full_schema |&gt;\n  mutate(\n    credit_util = total_credit_utilized / total_credit_limit,\n    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),\n    verified_income = droplevels(verified_income),\n    homeownership = str_to_title(homeownership),\n    homeownership = fct_relevel(homeownership, \"Rent\", \"Mortgage\", \"Own\")\n  ) |&gt;\n  rename(credit_checks = inquiries_last_12m) |&gt;\n  select(\n    interest_rate, loan_amount, verified_income, \n    debt_to_income, credit_util, bankruptcy, term, \n    credit_checks, issue_month, homeownership\n  )\n\nHere is a glimpse at the data:\n\nglimpse(loans)\n\nRows: 10,000\nColumns: 10\n$ interest_rate   &lt;dbl&gt; 14.07, 12.61, 17.09, 6.72, 14.07, 6.72, 13.59, 11.99, …\n$ loan_amount     &lt;int&gt; 28000, 5000, 2000, 21600, 23000, 5000, 24000, 20000, 2…\n$ verified_income &lt;fct&gt; Verified, Not Verified, Source Verified, Not Verified,…\n$ debt_to_income  &lt;dbl&gt; 18.01, 5.04, 21.15, 10.16, 57.96, 6.46, 23.66, 16.19, …\n$ credit_util     &lt;dbl&gt; 0.54759517, 0.15003472, 0.66134832, 0.19673228, 0.7549…\n$ bankruptcy      &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ term            &lt;dbl&gt; 60, 36, 36, 36, 36, 36, 60, 60, 36, 36, 60, 60, 36, 60…\n$ credit_checks   &lt;int&gt; 6, 1, 4, 0, 7, 6, 1, 1, 3, 0, 4, 4, 8, 6, 0, 0, 4, 6, …\n$ issue_month     &lt;fct&gt; Mar-2018, Feb-2018, Feb-2018, Jan-2018, Mar-2018, Jan-…\n$ homeownership   &lt;fct&gt; Mortgage, Rent, Rent, Rent, Rent, Own, Mortgage, Mortg…"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#goal",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#goal",
    "title": "Modeling loan interest rates",
    "section": "",
    "text": "Practice modeling with multiple predictors using the data on loan interest rates."
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#packages",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#packages",
    "title": "Modeling loan interest rates",
    "section": "",
    "text": "The dataset is about loans from the peer-to-peer lender, Lending Club, from the openintro package. We will use tidyverse and tidymodels for data exploration and modeling, respectively.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#data-prep",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#data-prep",
    "title": "Modeling loan interest rates",
    "section": "",
    "text": "Before we use the dataset, we’ll make a few transformations to it.\n\nReview the code below with your neighbor and write a summary of the data transformation pipeline.\n\nAdd response here.\n\nloans &lt;- loans_full_schema |&gt;\n  mutate(\n    credit_util = total_credit_utilized / total_credit_limit,\n    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),\n    verified_income = droplevels(verified_income),\n    homeownership = str_to_title(homeownership),\n    homeownership = fct_relevel(homeownership, \"Rent\", \"Mortgage\", \"Own\")\n  ) |&gt;\n  rename(credit_checks = inquiries_last_12m) |&gt;\n  select(\n    interest_rate, loan_amount, verified_income, \n    debt_to_income, credit_util, bankruptcy, term, \n    credit_checks, issue_month, homeownership\n  )\n\nHere is a glimpse at the data:\n\nglimpse(loans)\n\nRows: 10,000\nColumns: 10\n$ interest_rate   &lt;dbl&gt; 14.07, 12.61, 17.09, 6.72, 14.07, 6.72, 13.59, 11.99, …\n$ loan_amount     &lt;int&gt; 28000, 5000, 2000, 21600, 23000, 5000, 24000, 20000, 2…\n$ verified_income &lt;fct&gt; Verified, Not Verified, Source Verified, Not Verified,…\n$ debt_to_income  &lt;dbl&gt; 18.01, 5.04, 21.15, 10.16, 57.96, 6.46, 23.66, 16.19, …\n$ credit_util     &lt;dbl&gt; 0.54759517, 0.15003472, 0.66134832, 0.19673228, 0.7549…\n$ bankruptcy      &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ term            &lt;dbl&gt; 60, 36, 36, 36, 36, 36, 60, 60, 36, 36, 60, 60, 36, 60…\n$ credit_checks   &lt;int&gt; 6, 1, 4, 0, 7, 6, 1, 1, 3, 0, 4, 4, 8, 6, 0, 0, 4, 6, …\n$ issue_month     &lt;fct&gt; Mar-2018, Feb-2018, Feb-2018, Jan-2018, Mar-2018, Jan-…\n$ homeownership   &lt;fct&gt; Mortgage, Rent, Rent, Rent, Rent, Own, Mortgage, Mortg…"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#get-to-know-the-data",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#get-to-know-the-data",
    "title": "Modeling loan interest rates",
    "section": "Get to know the data",
    "text": "Get to know the data\n\nWhat is a typical interest rate in this dataset? What are some attributes of a typical loan and a typical borrower. Give yourself no more than 5 minutes for this exploration and share 1-2 findings.\n\n\n# add code here\n\n\n# add code here"
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#interest-rate-vs.-credit-utilization",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#interest-rate-vs.-credit-utilization",
    "title": "Modeling loan interest rates",
    "section": "Interest rate vs. credit utilization",
    "text": "Interest rate vs. credit utilization\n\nFor a regression model for predicting interest rate from credit utilization. Display the summary output.\n\n\n# add code here\n\n\nVisualize the model.\n\n\n# add code here\n\n\nInterpret the intercept and the slope.\n\nIntercept: Add response here.\nSlope: Add response here."
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#interest-rate-vs.-homeownership",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#interest-rate-vs.-homeownership",
    "title": "Modeling loan interest rates",
    "section": "Interest rate vs. homeownership",
    "text": "Interest rate vs. homeownership\n\nFit a regression model for predicting interest rate from homeownership and display the summary output.\n\n\n# add code here\n\n\n\nInterpret each coefficient in context of the problem.\n\nIntercept: Add response here.\n\nSlopes:\n\nAdd response here.\nAdd response here."
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#interest-rate-vs.-credit-utilization-and-homeownership",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#interest-rate-vs.-credit-utilization-and-homeownership",
    "title": "Modeling loan interest rates",
    "section": "Interest rate vs. credit utilization and homeownership",
    "text": "Interest rate vs. credit utilization and homeownership\nMain effects model\n\nFit a regression model to predict interest rate from credit utilization and homeownership, without an interaction effect between the two predictors. Display the summary output.\n\n\n# add code here\n\n\nWrite the estimated regression equation for loan applications from each of the homeownership groups separately.\n\nRent: \\(add~math~text~here\\)\n\nMortgage: \\(add~math~text~here\\)\n\nOwn: \\(add~math~text~here\\)\n\n\n\nHow does the model predict the interest rate to vary as credit utilization varies for loan applicants with different homeownership status. Are the rates the same or different?\n\nAdd response here.\nInteraction effects model\n\nFit a regression model to predict interest rate from credit utilization and homeownership, with an interaction effect between the two predictors. Display the summary output.\n\n\n# add code here\n\n\nWrite the estimated regression equation for loan applications from each of the homeownership groups separately.\n\nRent: \\(add~math~text~here\\)\n\nMortgage: \\(add~math~text~here\\)\n\nOwn: \\(add~math~text~here\\)\n\n\n\nHow does the model predict the interest rate to vary as credit utilization varies for loan applicants with different homeownership status. Are the rates the same or different?\n\nAdd response here.\nChoosing a model\nRule of thumb: Occam’s Razor - Don’t over-complicate the situation! We prefer the simplest best model.\n\nDisplay model level summary statistics.\n\n\n# add code here\n\n\nWhat is R-squared? What is adjusted R-squared?\n\nAdd response here.\n\nBased on the adjusted \\(R^2\\)s of these two models, which one do we prefer?\n\nAdd response here."
  },
  {
    "objectID": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#another-model-to-consider",
    "href": "code-alongs/2-2-modeling-loan-interest-rates/2-2-modeling-loan-interest-rates.html#another-model-to-consider",
    "title": "Modeling loan interest rates",
    "section": "Another model to consider",
    "text": "Another model to consider\n\nLet’s add one more model to the variable – issue month. Should we add this variable to the interaction effects model from earlier?\n\n\n# add code here\n\nAdd response here."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html",
    "title": "Modeling fish",
    "section": "",
    "text": "Practice modeling using the fish dataset on two common fish species in fish market sales.\n\nWe will use the tidyverse package for data wrangling and visualization and the tidymodels package for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nThese data come from Kaggle and is commonly used in machine learning examples.\n\nfish &lt;- read_csv(\"https://data-science-with-r.github.io/data/fish.csv\")\n\nThe data dictionary is below:\n\n\nvariable\ndescription\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\nLet’s take a look at the data.\n\nfish\n\n# A tibble: 55 × 7\n   species weight length_vertical length_diagonal length_cross height width\n   &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bream      242            23.2            25.4         30     11.5  4.02\n 2 Bream      290            24              26.3         31.2   12.5  4.31\n 3 Bream      340            23.9            26.5         31.1   12.4  4.70\n 4 Bream      363            26.3            29           33.5   12.7  4.46\n 5 Bream      430            26.5            29           34     12.4  5.13\n 6 Bream      450            26.8            29.7         34.7   13.6  4.93\n 7 Bream      500            26.8            29.7         34.5   14.2  5.28\n 8 Bream      390            27.6            30           35     12.7  4.69\n 9 Bream      450            27.6            30           35.1   14.0  4.84\n10 Bream      500            28.5            30.7         36.2   14.2  4.96\n# ℹ 45 more rows"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#goal",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#goal",
    "title": "Modeling fish",
    "section": "",
    "text": "Practice modeling using the fish dataset on two common fish species in fish market sales."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#packages",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#packages",
    "title": "Modeling fish",
    "section": "",
    "text": "We will use the tidyverse package for data wrangling and visualization and the tidymodels package for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#data",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#data",
    "title": "Modeling fish",
    "section": "",
    "text": "These data come from Kaggle and is commonly used in machine learning examples.\n\nfish &lt;- read_csv(\"https://data-science-with-r.github.io/data/fish.csv\")\n\nThe data dictionary is below:\n\n\nvariable\ndescription\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\nLet’s take a look at the data.\n\nfish\n\n# A tibble: 55 × 7\n   species weight length_vertical length_diagonal length_cross height width\n   &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bream      242            23.2            25.4         30     11.5  4.02\n 2 Bream      290            24              26.3         31.2   12.5  4.31\n 3 Bream      340            23.9            26.5         31.1   12.4  4.70\n 4 Bream      363            26.3            29           33.5   12.7  4.46\n 5 Bream      430            26.5            29           34     12.4  5.13\n 6 Bream      450            26.8            29.7         34.7   13.6  4.93\n 7 Bream      500            26.8            29.7         34.5   14.2  5.28\n 8 Bream      390            27.6            30           35     12.7  4.69\n 9 Bream      450            27.6            30           35.1   14.0  4.84\n10 Bream      500            28.5            30.7         36.2   14.2  4.96\n# ℹ 45 more rows"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#visualizing-the-model",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#visualizing-the-model",
    "title": "Modeling fish",
    "section": "Visualizing the model",
    "text": "Visualizing the model\nWe’re going to investigate the relationship between the weights and heights of fish, predicting weight from height.\n\nCreate an appropriate plot to investigate this relationship. Add appropriate labels to the plot.\n\n\n# add code here\n\n\nIf you were to draw a a straight line to best represent the relationship between the heights and weights of fish, where would it go? Why?\n\nAdd response here.\n\nNow, let R draw the line for you! Hint: Use geom_smooth().\n\n\n# add code here\n\n\nWhat types of questions can this plot help answer?\n\nAdd response here.\n\nWe can use this line to make predictions. Predict what you think the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm. Which prediction is considered extrapolation?\n\nAdd response here.\n\nWhat is a residual?\n\nAdd response here."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#model-fitting",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#model-fitting",
    "title": "Modeling fish",
    "section": "Model fitting",
    "text": "Model fitting\n\nFit a model to predict fish weights from their heights.\n\n\n# add code here\n\n\nPredict what the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm using this model.\n\n\n# add code here\n\n\nCalculate predicted weights for all fish in the data and visualize the residuals under this model.\n\n\n# add code here"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#model-summary",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#model-summary",
    "title": "Modeling fish",
    "section": "Model summary",
    "text": "Model summary\n\nDisplay the model summary including estimates for the slope and intercept along with measurements of uncertainty around them. Show how you can extract these values from the model output.\n\n\n# add code here\n\n\nWrite out your model using mathematical notation.\n\nAdd response here."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#correlation",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#correlation",
    "title": "Modeling fish",
    "section": "Correlation",
    "text": "Correlation\nWe can also assess correlation between two quantitative variables.\n\nWhat is correlation? What are values correlation can take?\n\nAdd response here.\n\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\nWhat is the correlation between heights and weights of fish?\n\n\n# add code here"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#adding-a-third-variable",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#adding-a-third-variable",
    "title": "Modeling fish",
    "section": "Adding a third variable",
    "text": "Adding a third variable\n\nDoes the relationship between heights and weights of fish change if we take into consideration species? Plot two separate straight lines for the Bream and Roach species.\n\n\n# add code here"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#fitting-other-models",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#fitting-other-models",
    "title": "Modeling fish",
    "section": "Fitting other models",
    "text": "Fitting other models\n\nWe can fit more models than just a straight line. Use method = \"loess\". What is different from the plot created before?\n\n\n# add code here"
  }
]