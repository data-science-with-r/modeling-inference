[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n Modeling and inference",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nLanguage of models\n\n\n\n\n\nLinear models with a single predictor\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "\n Modeling and inference",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nLanguage of models\n\n\n\n\n\nLinear models with a single predictor\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#code-alongs",
    "href": "index.html#code-alongs",
    "title": "\n Modeling and inference",
    "section": "Code alongs",
    "text": "Code alongs\n\n\n\nTitle\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "\n Modeling and inference",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nTitle\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#transform---visualize",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#transform---visualize",
    "title": "Welcome",
    "section": "Transform <-> visualize",
    "text": "Transform &lt;-&gt; visualize"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#import---tidy",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#import---tidy",
    "title": "Welcome",
    "section": "Import <-> tidy",
    "text": "Import &lt;-&gt; tidy"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#data-science-ethics",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#data-science-ethics",
    "title": "Welcome",
    "section": "Data science ethics",
    "text": "Data science ethics\n\nMisrepresentation\nData privacy\nAlgorithmic bias"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#modeling-and-inference",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#modeling-and-inference",
    "title": "Welcome",
    "section": "Modeling and inference",
    "text": "Modeling and inference\n\n\n\n\n\n\n\n\n\nFitting, interpreting, selecting, and evaluating models\nPrediction, classification, and assessing accuracy\nMaking inferences and quantifying uncertainty"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling",
    "title": "Language of models",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but remember there are many many other types of models too!)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-cars",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-cars",
    "title": "Language of models",
    "section": "Modeling cars",
    "text": "Modeling cars\n\n\nWhat is the relationship between cars’ weights and their mileage?\nWhat is your best guess for a car’s MPG that weighs 3,500 pounds?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars",
    "title": "Language of models",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nDescribe: What is the relationship between cars’ weights and their mileage?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars-1",
    "title": "Language of models",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-1",
    "title": "Language of models",
    "section": "Modeling",
    "text": "Modeling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but there are many many other types of models too!)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-vocabulary",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-vocabulary",
    "title": "Language of models",
    "section": "Modeling vocabulary",
    "text": "Modeling vocabulary\n\nOutcome: Variable whose behavior or variation you are trying to understand, on the y-axis (aka response variable, dependent variable)\nPredictor(s): Other variable(s) that you want to use to explain the variation in the outcome, on the x-axis (aka explanatory variable(s), independent variable(s))\nModel function: The regression line for predicting the outcome variable from the predictor variable(s), comprised generally of an intercept and a slope for each predictor\nPredicted value: Output of the model function, which gives the typical (expected) value of the outcome conditioning on the predictor\n\nResiduals: A measure of how far each case’s observed value is from its predicted value (based on a particular model)\n\nResidual = Observed value - Predicted value\nTells how far above/below the expected value each case is"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#predictor",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#predictor",
    "title": "Language of models",
    "section": "Predictor",
    "text": "Predictor\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#outcome",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#outcome",
    "title": "Language of models",
    "section": "Outcome",
    "text": "Outcome\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line",
    "title": "Language of models",
    "section": "Regression line",
    "text": "Regression line"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-slope",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-slope",
    "title": "Language of models",
    "section": "Regression line: slope",
    "text": "Regression line: slope"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-intercept",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-intercept",
    "title": "Language of models",
    "section": "Regression line: intercept",
    "text": "Regression line: intercept"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation",
    "title": "Language of models",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation-1",
    "title": "Language of models",
    "section": "Correlation",
    "text": "Correlation\n\nRanges between -1 and 1.\nSame sign as the slope."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#visualizing-the-model",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#visualizing-the-model",
    "title": "Language of models",
    "section": "Visualizing the model",
    "text": "Visualizing the model\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#residuals",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#residuals",
    "title": "Language of models",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#extending-regression-lines",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#extending-regression-lines",
    "title": "Language of models",
    "section": "Extending regression lines",
    "text": "Extending regression lines"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#models---upsides-and-downsides",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#models---upsides-and-downsides",
    "title": "Language of models",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modeling over simple visual inspection of data.\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#variation-around-the-model",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#variation-around-the-model",
    "title": "Language of models",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\nStatistics is the explanation of variation in the context of what remains unexplained.\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#how-do-we-use-models",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#how-do-we-use-models",
    "title": "Language of models",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nPredict / classify: Plug in the value(s) of predictor(s) to the model to obtain the predicted value of the outcome\nDescribe: Quantify the relationship between predictor(s) and outcome with slopes"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#self-driving-cars",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#self-driving-cars",
    "title": "Language of models",
    "section": "Self driving cars",
    "text": "Self driving cars"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#semi-or-garage",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#semi-or-garage",
    "title": "Language of models",
    "section": "Semi or garage?",
    "text": "Semi or garage?\n\ni love how Tesla thinks the wall in my garage is a semi. 😅\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#semi-or-garage-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#semi-or-garage-1",
    "title": "Language of models",
    "section": "Semi or garage?",
    "text": "Semi or garage?\n\nNew owner here. Just parked in my garage. Tesla thinks I crashed onto a semi.\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#car-or-trash",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#car-or-trash",
    "title": "Language of models",
    "section": "Car or trash?",
    "text": "Car or trash?\n\nTesla calls Mercedes trash\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp",
    "title": "Language of models",
    "section": "Leisure, commute, physical activity and BP",
    "text": "Leisure, commute, physical activity and BP\n\nRelation Between Leisure Time, Commuting, and Occupational Physical Activity With Blood Pressure in 125,402 Adults: The Lifelines Cohort\nByambasukh, Oyuntugs, Harold Snieder, and Eva Corpeleijn. “Relation between leisure time, commuting, and occupational physical activity with blood pressure in 125 402 adults: the lifelines cohort.” Journal of the American Heart Association 9.4 (2020): e014313."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp-1",
    "title": "Language of models",
    "section": "Leisure, commute, physical activity and BP",
    "text": "Leisure, commute, physical activity and BP\nBackground: Whether all domains of daily‐life moderate‐to‐vigorous physical activity (MVPA) are associated with lower blood pressure (BP) and how this association depends on age and body mass index remains unclear.\nMethods and Results: In the population‐based Lifelines cohort (N=125,402), MVPA was assessed by the Short Questionnaire to Assess Health‐Enhancing Physical Activity, a validated questionnaire in different domains such as commuting, leisure‐time, and occupational PA. BP was assessed using the last 3 of 10 measurements after 10 minutes’ rest in the supine position. Hypertension was defined as systolic BP ≥140 mm Hg and/or diastolic BP ≥90 mm Hg and/or use of antihypertensives. In regression analysis, higher commuting and leisure‐time but not occupational MVPA related to lower BP and lower hypertension risk. Commuting‐and‐leisure‐time MVPA was associated with BP in a dose‐dependent manner. β Coefficients (95% CI) from linear regression analyses were −1.64 (−2.03 to −1.24), −2.29 (−2.68 to −1.90), and finally −2.90 (−3.29 to −2.50) mm Hg systolic BP for the low, middle, and highest tertile of MVPA compared with “No MVPA” as the reference group after adjusting for age, sex, education, smoking and alcohol use. Further adjustment for body mass index attenuated the associations by 30% to 50%, but more MVPA remained significantly associated with lower BP and lower risk of hypertension. This association was age dependent. β Coefficients (95% CI) for the highest tertiles of commuting‐and‐leisure‐time MVPA were −1.67 (−2.20 to −1.15), −3.39 (−3.94 to −2.82) and −4.64 (−6.15 to −3.14) mm Hg systolic BP in adults &lt;40, 40 to 60, and &gt;60 years, respectively.\nConclusions: Higher commuting and leisure‐time but not occupational MVPA were significantly associated with lower BP and lower hypertension risk at all ages, but these associations were stronger in older adults."
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#packages",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#packages",
    "title": "Linear models with a single predictor",
    "section": "Packages",
    "text": "Packages\n\n\nfivethirtyeight for data\n\ntidyverse for data wrangling and visualization\n\ntidymodels for modeling\n\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#data-prep",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#data-prep",
    "title": "Linear models with a single predictor",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\n\nRename the dataset as movie_scores\n\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#data-overview",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#data-overview",
    "title": "Linear models with a single predictor",
    "section": "Data overview",
    "text": "Data overview\n\nmovie_scores |&gt;\n  select(critics, audience)\n\n# A tibble: 146 × 2\n   critics audience\n     &lt;int&gt;    &lt;int&gt;\n 1      74       86\n 2      85       80\n 3      80       90\n 4      18       84\n 5      14       28\n 6      63       62\n 7      42       53\n 8      86       64\n 9      99       82\n10      89       87\n# ℹ 136 more rows"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#data-visualization",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#data-visualization",
    "title": "Linear models with a single predictor",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#regression-model-1",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#regression-model-1",
    "title": "Linear models with a single predictor",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the outcome, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#regression-model",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#regression-model",
    "title": "Linear models with a single predictor",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{#325b74}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{#325b74}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{#325b74}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#simple-linear-regression",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#simple-linear-regression",
    "title": "Linear models with a single predictor",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nUse simple linear regression to model the relationship between a quantitative outcome (\\(Y\\)) and a single quantitative predictor (\\(X\\)): \\[\\Large{Y = \\beta_0 + \\beta_1 X + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(\\epsilon\\): Error (residual)"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#simple-linear-regression-1",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#simple-linear-regression-1",
    "title": "Linear models with a single predictor",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\\[\\Large{\\hat{Y} = b_0 + b_1 X}\\]\n\n\n\\(b_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(b_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\n\nNo error term!"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#choosing-values-for-b_1-and-b_0",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#choosing-values-for-b_1-and-b_0",
    "title": "Linear models with a single predictor",
    "section": "Choosing values for \\(b_1\\) and \\(b_0\\)\n",
    "text": "Choosing values for \\(b_1\\) and \\(b_0\\)"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#residuals",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#residuals",
    "title": "Linear models with a single predictor",
    "section": "Residuals",
    "text": "Residuals\n\n\n\n\n\n\n\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}\\]"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#least-squares-line",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#least-squares-line",
    "title": "Linear models with a single predictor",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#least-squares-line-1",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#least-squares-line-1",
    "title": "Linear models with a single predictor",
    "section": "Least squares line",
    "text": "Least squares line\n\nmovies_fit &lt;- linear_reg() |&gt;\n  fit(audience ~ critics, data = movie_scores)\n\ntidy(movies_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#interpreting-the-slope",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#interpreting-the-slope",
    "title": "Linear models with a single predictor",
    "section": "Interpreting the slope",
    "text": "Interpreting the slope\n\nThe slope of the model for predicting audience score from critics score is 0.519. Which of the following is the best interpretation of this value?\n\n\nFor every one point increase in the critics score, the audience score goes up by 0.519 points, on average.\nFor every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\nFor every one point increase in the critics score, the audience score goes up by 0.519 points.\nFor every one point increase in the audience score, the critics score goes up by 0.519 points, on average."
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#interpreting-slope-intercept",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#interpreting-slope-intercept",
    "title": "Linear models with a single predictor",
    "section": "Interpreting slope & intercept",
    "text": "Interpreting slope & intercept\n\\[\\widehat{\\text{audience}} = 32.3 + 0.519 \\times \\text{critics}\\]\n\n\nSlope: For every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\n\nIntercept: If the critics score is 0 points, we expect the audience score to be 32.3 points."
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#is-the-intercept-meaningful",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#is-the-intercept-meaningful",
    "title": "Linear models with a single predictor",
    "section": "Is the intercept meaningful?",
    "text": "Is the intercept meaningful?\n✅ The intercept is meaningful in context of the data if\n\nthe predictor can feasibly take values equal to or near zero or\nthe predictor has values near zero in the observed data\n\n\n🛑 Otherwise, it might not be meaningful!"
  },
  {
    "objectID": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#properties-of-least-squares-regression",
    "href": "slides/1-2-1-lin-mod-single-predictor/1-2-1-lin-mod-single-predictor.html#properties-of-least-squares-regression",
    "title": "Linear models with a single predictor",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point (the coordinates corresponding to average \\(X\\) and average \\(Y\\)): \\(b_0 = \\bar{Y} - b_1~\\bar{X}\\)\nSlope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_Y}{s_X}\\)\nSum of the residuals is zero: \\(\\sum_{i = 1}^n \\epsilon_i = 0\\)\nResiduals and \\(X\\) values are uncorrelated"
  }
]