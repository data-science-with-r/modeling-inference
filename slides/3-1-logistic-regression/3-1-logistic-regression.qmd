---
title: "Logistic regression"
format: revealjs
---

```{r}
#| label: setup
#| include: false
ggplot2::theme_set(ggplot2::theme_gray(base_size = 16))
```

## Packages

- **tidyverse** for data wrangling and visualization
- **tidymodels** for modeling

```{r}
#| label: load-packages
#| warning: false
library(tidyverse)
library(tidymodels)
library(ggthemes)
library(openintro)
library(fivethirtyeight)
library(palmerpenguins)
library(MASS)
```

# Regression so far

## Recap: Simple linear regression {.smaller}

Numerical response and one numerical predictor:

```{r}
#| label: movies
#| message: false
#| echo: false
movie_scores <- fandango |>
  rename(
    critics = rottentomatoes, 
    audience = rottentomatoes_user
  )
ggplot(movie_scores, aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE, linewidth = 1.5) + 
  labs(
    x = "Critics score" , 
    y = "Audience score",
    title = "Rotten Tomatoes scores"
  )
```

## Recap: Multiple linear regression {.smaller}

Numerical outcome and one categorical predictor (two levels):

```{r}
#| label: smoking
#| message: false
#| echo: false
set.seed(1234)
births14 |>
  filter(!is.na(habit)) |>
  ggplot(aes(x = habit, y = weight)) +
  geom_jitter(alpha = 0.3) +
  geom_smooth(aes(group = 1), method = "lm", se = FALSE, linewidth = 1.5) +
  scale_x_discrete(labels = c("Non-smoker (0)", "Smoker (1)")) +
  labs(
    x = "Habit",
    y = "Birth weight (lbs)",
    title = "Smoking during pregnancy"
  )
```

## Recap: multiple linear regression {.smaller}

Numerical outcome, numerical and categorical predictors:

```{r}
#| label: penguins
#| echo: false
#| warning: false
bm_fl_island_fit <- linear_reg() |>
  fit(body_mass_g ~ flipper_length_mm + island, data = penguins)
bm_fl_island_aug <- augment(bm_fl_island_fit, new_data = penguins)
ggplot(
  bm_fl_island_aug, 
  aes(x = flipper_length_mm, y = body_mass_g, color = island)
  ) +
  geom_point(alpha = 0.5) +
  geom_smooth(aes(y = .pred), method = "lm") +
  labs(
    title = "Penguins dataset from AE 14",
    x = "Flipper length (mm)",
    y = "Body mass (g)",
    color = "Island"
    ) +
  scale_color_colorblind() +
  theme(legend.position = "bottom")
```

# New scenario: *binary* response

## A *binary* response {.smaller}

$$
y = 
\begin{cases}
1 & &&\text{eg. Yes, Win, True, Heads, Success}\\
0 & &&\text{eg. No, Lose, False, Tails, Failure}.
\end{cases}
$$

```{r}
#| label: sample-df-settings
#| echo: false
set.seed(8675309)
n <- 50
b0 <- 0  
b1 <- 5
df <- tibble(
  x = rnorm(n, mean = 0, sd = 1),
  prob = 1 / (1 + exp(-(b0 + b1 * x))),
  y = as.factor(rbinom(n, 1, prob))
)
fit <- logistic_reg() |>
  fit(y ~ x, data = df)

aug <- augment(fit, new_data = df)
```

```{r}
#| label: sample-df-S-curve
#| echo: false
ggplot(aug, aes(x = x)) +
  geom_line(aes(y = .pred_1), color = "blue", linewidth = 1.5, alpha = 0.5) +
  geom_point(aes(y = as.numeric(y) - 1)) +
  labs(
    x = "X",
    y = "P(Y = 1)"
  ) +
  scale_y_continuous(breaks = c(0, 1))
```

## Who cares?

If we can model the relationship between predictors ($x$) and a binary response ($y$), we can use the model to do a special kind of prediction called *classification*.

## Example: Is the e-mail spam or not? {.smaller}

::: {.columns}
::: {.column width="20%"}

$\mathbf{x}$: Word and character counts in an e-mail

:::
::: {.column width="50%"}

$$
y
= 
\begin{cases}
1 & \text{it's spam}\\
0 & \text{it's legit}
\end{cases}
$$

:::
:::

::: {.columns}
::: {.column width=70%}
::: email
Subject: Congratulations! Youâ€™ve Been Selected for an Exclusive Reward ðŸŽ

Dear Customer,

You have been chosen as one of our preferred recipients to receive a special complimentary gift. This is our way of thanking you for your continued interest in our services.

To claim your reward, simply complete our short survey. Your participation takes only 60 seconds, and your prize will be shipped at no cost to you.

Click here to start your survey and claim your reward
[Claim Reward Link]

This exclusive offer is available for the next 48 hours only. Donâ€™t miss your chance to enjoy this limited opportunity.

Warm regards,  
Promotions Team  
Exclusive Rewards Center
:::
:::
::: {.column width=30%}
:::
:::

::: aside
Sample spam email language generated by Chat GPT with the prompt "Generate a fake â€œpromotionalâ€ style spam email that doesn't contain any explicit words."
:::

## Example: Is it cancer or not? {.smaller}

::: {.columns}
::: {.column width="20%"}

$\mathbf{x}$: features in a medical image

:::
::: {.column width="50%"}

$$
y
= 
\begin{cases}
1 & \text{it's cancer}\\
0 & \text{it's healthy}
\end{cases}
$$

:::
:::

![](images/national-cancer-institute-BDKid0yJcAk-unsplash.png){fig-align="center" width=600px}


::: aside
Photo by <a href="https://unsplash.com/@nci?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">National Cancer Institute</a> on <a href="https://unsplash.com/photos/a-black-and-white-photo-of-various-mri-images-BDKid0yJcAk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>.
:::

## Example: Will they default? {.smaller}

::: {.columns}
::: {.column width="20%"}

$\mathbf{x}$: financial and demographic info about a loan applicant

:::
::: {.column width="50%"}

$$
y
= 
\begin{cases}
1 & \text{applicant is at risk of defaulting on loan}\\
0 & \text{applicant is safe}
\end{cases}
$$

:::
:::

![](images/credit-card-6401275_640.png){fig-align="center" width=600px}

::: aside
Photo by <a href="https://pixabay.com/illustrations/credit-card-credit-score-mastercard-6401275/">PabitraKaity</a> on <a href="https://pixabay.com/illustrations/credit-card-credit-score-mastercard-6401275/">Pixabay</a>.
:::

## Example: Will they reoffend? {.smaller}

::: {.columns}
::: {.column width="20%"}

$\mathbf{x}$: info about a criminal suspect and their case

:::
::: {.column width="50%"}

$$
y
= 
\begin{cases}
1 & \text{suspect is at risk of re-offending pre-trial}\\
0 & \text{suspect is safe}
\end{cases}
$$

:::
:::

![](images/machine-bias-petty-theft-1.png){fig-align="center" width=400px} ![](images/machine-bias-drug-posession-1.png){fig-align="center" width=400px}

::: aside
[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) on ProPublica.
:::

## How do we model this type of data?

```{r}
#| label: sample-df-data
#| message: false
#| echo: false
ggplot(aug, aes(x = x)) +
  geom_point(aes(y = as.numeric(y) - 1)) +
  labs(
    x = "X",
    y = "Y"
  ) +
  scale_y_continuous(breaks = c(0, 1))
```

## Straight line of best fit is a little silly

```{r}
#| label: sample-df-line
#| message: false
#| echo: false
ggplot(aug, aes(x = x, y = as.numeric(y) - 1)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    x = "X",
    y = "Y"
  ) +
  scale_y_continuous(breaks = c(0, 1))
```

## Instead: S-curve of best fit {.smaller}

Instead of modeling $y$ directly, we model the probability that $y=1$:

```{r}
#| label: sample-df-S-curve-again
#| ref-label: "sample-df-S-curve"
#| echo: false
```

-   "Given new email, what's the probability that it's spam?''
-   "Given new image, what's the probability that it's cancer?''
-   "Given new loan application, what's the probability that they default?''

## Why don't we model y directly?

-   **Recall regression with a numerical response**:

    -   Our models do not output *guarantees* for $y$, they output predictions that describe behavior *on average*;

-   **Similar when modeling a binary response**:

    -   Our models cannot directly guarantee that $y$ will be zero or one. The correct analog to "on average" for a 0/1 response is "what's the probability?"

## So, what is this S-curve, anyway?

It's the **logistic function**:

$$
\text{Prob}(y = 1)
=
\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}.
$$

If you set p = Prob(y = 1) and do some algebra, you get the simple linear model for the *log-odds*:

$$
\log\left(\frac{p}{1-p}\right)
=
\beta_0+\beta_1x.
$$

This is called the *logistic regression* model.

## Log-odds?

-   $p = Prob(y = 1)$ is a probability -- A number between 0 and 1

-   $p / (1 - p)$ is the odds -- A number between 0 and $\infty$

-   The log odds $log(p / (1 - p))$ is a number between $-\infty$ and $\infty$, which is suitable for the linear model

## Logistic regression

$$
\log\left(\frac{p}{1-p}\right)
=
\beta_0+\beta_1x
$$

-   The *logit* function $log(p / (1-p))$ is an example of a **link function** that transforms the linear model to have an appropriate range

-   This is an example of a **generalized linear model**

## Estimation

-   We estimate the parameters $\beta_0$, $\beta_1$, etc. using *maximum likelihood* (don't worry about it) to get the "best fitting" S-curve;

-   The fitted model is

$$
\log\left(\frac{\widehat{p}}{1-\widehat{p}}\right)
=
b_0+b_1x
$$

# Classification

## Step 1: Pick a threshold {.smaller}

Select a number $0 < p^* < 1$:

```{r}
#| label: step-1
#| message: false
#| echo: false
threshold <- 0.85
step_1 <- ggplot(aug, aes(x = x)) +
  geom_line(aes(y = .pred_1), color = "blue", linewidth = 1.5, alpha = 0.5) +
  geom_point(aes(y = as.numeric(y) - 1)) +
  labs(
    x = "x",
    y = "Prob(y = 1)"
  ) +
  scale_y_continuous(breaks = c(0, 1)) +
  geom_hline(yintercept = threshold, linetype = "dotted") +
  annotate(
    "label",
    x = -1.75,
    y = threshold,
    label = "p* = 0.85",
    vjust = -0.1,
    hjust = 0.1
  )

step_1
```

-   if $\text{Prob}(y=1)\leq p^*$, then predict $\widehat{y}=0$
-   if $\text{Prob}(y=1)> p^*$, then predict $\widehat{y}=1$

## Step 2: Find the "decision boundary" {.smaller}

Solve for the x-value that matches the threshold:

```{r}
#| label: step-2
#| message: false
#| echo: false
cutoff <- (log(threshold / (1 - threshold)) - tidy(fit)$estimate[1]) / tidy(fit)$estimate[2]

step_2 <- step_1 +
  geom_vline(xintercept = cutoff, linetype = "dotted") +
  annotate(
    "label",
    x = cutoff,
    y = 0.1,
    label = "x*",
    hjust = -0.1
  )

step_2
```

-   if $\text{Prob}(y=1)\leq p^*$, then predict $\widehat{y}=0$;
-   if $\text{Prob}(y=1)> p^*$, then predict $\widehat{y}=1$.

## Step 3: Classify a new arrival {.smaller}

A new person shows up with $x_{\text{new}}$.
Which side of the boundary are they on?

```{r}
#| label: step-3
#| message: false
#| echo: false
step_2 +
  annotate(
    "label",
    x = 1,
    y = 0.5,
    label = paste("hat(y)", "== 1"),
    parse = TRUE,
    fill = "lightblue"
  ) +
  annotate(
    "label",
    x = -1,
    y = 0.5,
    label = paste("hat(y)", "== 0"),
    parse = TRUE,
    fill = "peachpuff"
  )
```

-   if $x_{\text{new}} \leq x^\star$, then $\text{Prob}(y=1)\leq p^*$, so predict $\widehat{y}=0$ for the new person;
-   if $x_{\text{new}} > x^\star$, then $\text{Prob}(y=1)> p^*$, so predict $\widehat{y}=1$ for the new person.

## Let's change the threshold {.smaller}

A new person shows up with $x_{\text{new}}$.
Which side of the boundary are they on?

```{r}
#| label: lower-threshold
#| message: false
#| echo: false
threshold <- 0.15
cutoff <- (log(threshold / (1 - threshold)) - tidy(fit)$estimate[1]) / tidy(fit)$estimate[2]

step_1 +
  geom_vline(xintercept = cutoff, linetype = "dotted") +
  annotate(
    "label",
    x = cutoff,
    y = 0.1,
    label = "x*",
    hjust = -0.1
  ) +
  annotate(
    "label",
    x = 1,
    y = 0.5,
    label = paste("hat(y)", "== 1"),
    parse = TRUE,
    fill = "lightblue"
  ) +
  annotate(
    "label",
    x = -1,
    y = 0.5,
    label = paste("hat(y)", "== 0"),
    parse = TRUE,
    fill = "peachpuff"
  )
```

-   if $x_{\text{new}} \leq x^\star$, then $\text{Prob}(y=1)\leq p^*$, so predict $\widehat{y}=0$ for the new person;
-   if $x_{\text{new}} > x^\star$, then $\text{Prob}(y=1)> p^*$, so predict $\widehat{y}=1$ for the new person.

## Nothing special about one predictor... {.smaller}

Two numerical predictors and one binary response:

```{r}
#| message: false
#| echo: false
set.seed(20)
n <- 20
cloud1 <- mvrnorm(n, c(-0.5, -0.5), matrix(c(1, 0.5, 0.5, 1), 2, 2))
cloud2 <- mvrnorm(n, c(0.5, 0.5), matrix(c(1, -0.5, -0.5, 1), 2, 2))
df_new <- tibble(
  x1 = c(cloud1[,1], cloud2[,1]),
  x2 = c(cloud1[,2], cloud2[,2]),
  y = as.factor(c(rep(0, n), rep(1, n)))
)

step_1_new <- ggplot(df_new, aes(x = x1, y = x2, color = y)) +
  geom_point() +
  scale_color_manual(values = c("orange", "blue"), labels = c("y = 0", "y = 1")) +
  labs(color = NULL) +
  theme(
    legend.position = "inside",
    legend.position.inside = c(0.85, 0.15)
  )

step_1_new
```

## "Multiple" logistic regression

On the probability scale:

$$
\text{Prob}(y = 1)
=
\frac{e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}}{1+e^{\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m}}.
$$

For the log-odds, a *multiple* linear regression:

$$
\log\left(\frac{p}{1-p}\right)
=
\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m.
$$

## Decision boundary, again {.smaller}

It's linear!
Consider two numerical predictors:

```{r}
#| message: false
#| echo: false
fit_new <- logistic_reg() |>
  fit(y ~ x1 + x2, data = df_new)

b0 <- tidy(fit_new)$estimate[1]
b1 <- tidy(fit_new)$estimate[2]
b2 <- tidy(fit_new)$estimate[3]

p_thresh <- 0.5

# compute intercept and slope of decision boundary
bd_incpt <- (log(p_thresh / (1 - p_thresh)) - b0) / b2
bd_slp <- -b1 / b2

step_1_new +
  geom_abline(yintercept = bd_incpt, slope = bd_slp, color = "blue", linetype = "dashed") +
  annotate(
    "label",
    x = -1.5,
    y = 1.75,
    label = paste("hat(y)", "== 1"),
    parse = TRUE,
    fill = "lightblue"
  ) +
  annotate(
    "label",
    x = -2,
    y = 0.5,
    label = paste("hat(y)", "== 0"),
    parse = TRUE,
    fill = "peachpuff"
  )
```

-   if new $(x_1,\,x_2)$ below, $\text{Prob}(y=1)\leq p^*$. Predict $\widehat{y}=0$ for the new person;
-   if new $(x_1,\,x_2)$ above, $\text{Prob}(y=1)> p^*$. Predict $\widehat{y}=1$ for the new person.
