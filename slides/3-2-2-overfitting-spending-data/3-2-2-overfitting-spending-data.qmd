---
title: "Overfitting and spending your data"
format: revealjs
---

```{r}
#| label: setup
#| include: false
ggplot2::theme_set(ggplot2::theme_gray(base_size = 16))
```

# Setup

## Packages

- **tidyverse** for data wrangling and visualization
- **tidymodels** for modeling
- **forested** for data

```{r}
#| label: load-packages
#| message: false
library(tidyverse)
library(tidymodels)
library(forested)
```

## Data

-   The U.S. Forest Service maintains machine learning models to predict whether a plot of land is "forested."

-   This classification is important for research, legislation, land management, etc. purposes.

-   Plots are typically remeasured every 10 years.

-   The `forested` dataset contains the most recent measurement per plot.

## Data: `forested`

```{r}
forested
```

## Data: `forested`

```{r}
glimpse(forested)
```

## Data: `forested`

```{r}
names(forested)
```

## Outcome and predictors 

-   Outcome: `forested` - Factor, `Yes` or `No`

```{r}
levels(forested$forested)
```

. . .

-   Predictors: 18 remotely-sensed and easily-accessible variables:

    -   numeric variables based on weather and topography

    -   categorical variables based on classifications from other governmental organizations

## `?forested`

<iframe width="900" height="500" src="https://simonpcouch.github.io/forested/reference/forested.html" title="forested" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Should we include a predictor?

To determine whether we should include a predictor in a model, we should start by asking:

::: incremental
-   Is it ethical to use this variable?
    (Or even legal?)

-   Will this variable be available at prediction time?

-   Does this variable contribute to explainability?
:::

# Data splitting and spending

## We've been cheating!

::: incremental
-   So far, we've been using all the data we have for building models.
    In predictive contexts, this would be considered *cheating*.

-   Evaluating model performance for predicting outcomes that were used when building the models is like evaluating your learning with questions whose answers you've already seen.
:::

## Spending your data {.smaller}

For predictive models (used primarily in machine learning), we typically split data into training and test sets:

![](images/test-train-split.png){fig-align="center"}

. . .

-   The **training set** is used to estimate model parameters.

-   The **test set** is used to find an independent assessment of model performance.

. . .

::: {.columns}
::: {.column width=70%}
::: callout-warning
Do not use, or even peek at, the test set during training.
:::
:::
::: {.column}
:::
::: 

## How much to spend?

::: incremental
-   The more data we spend (use in training), the better estimates we’ll get.

-   Spending too much data in training prevents us from computing a good assessment of predictive performance.

-   Spending too much data in testing prevents us from computing a good estimate of model parameters.
:::

## The initial split

The default split is 75% training, 25% testing.

```{r}
#| label: initial-split
set.seed(20241112)
forested_split <- initial_split(forested)
forested_split
```

## Setting a seed

::: {.columns}
::: {.column width=70%}
::: task
What does `set.seed()` do?
:::
:::
::: {.column}
:::
:::


::: incremental
-   To create that split of the data, R generates “pseudo-random” numbers: while they are made to behave like random numbers, their generation is deterministic given a “seed”.

-   This allows us to reproduce results by setting that seed.

-   Which seed you pick doesn’t matter, as long as you don’t try a bunch of seeds and pick the one that gives you the best performance.
:::

## Accessing the data

```{r}
#| label: access-data
forested_train <- training(forested_split)
forested_test <- testing(forested_split)
```

## The training set

```{r}
forested_train
```

## The testing data

```{r}
#| eval: false
forested_test
```

🙈

. . .

```{r}
dim(forested_test)
```

# Exploratory data analysis

## Initial questions

-   What’s the distribution of the outcome, `forested`?

-   What’s the distribution of numeric variables like `precip_annual`?

-   How does the distribution of forested differ across the categorical and numerical variables?

. . .

::: {.columns}
::: {.column width=70%}
::: task
Which dataset should we use for the exploration?
The entire data `forested`, the training data `forested_train`, or the testing data `forested_test`?
:::
:::
::: {.column}
:::
:::

## `forested`

::: {.columns}
::: {.column width=70%}
::: task
What’s the distribution of the outcome, `forested`?
:::
:::
::: {.column}
:::
:::

```{r}
forested_train |>
  count(forested) |>
  mutate(p = n / sum(n))
```

## `precip_annual`

What’s the distribution of `precip_annual`?

```{r}
ggplot(forested_train, aes(x = precip_annual)) +
  geom_histogram(binwidth = 200)
```

## `forested` and `precip_annual` {.smaller}

```{r}
ggplot(
  forested_train,
  aes(x = precip_annual, fill = forested, group = forested)
  ) +
  geom_histogram(binwidth = 200, position = "identity", alpha = 0.7) +
  scale_fill_manual(values = c("Yes" = "forestgreen", "No" = "gold2")) +
  theme_minimal()
```

## `forested` and `precip_annual` {.smaller}

```{r}
#| warning: false
ggplot(
  forested_train,
  aes(x = precip_annual, fill = forested, group = forested)
  ) +
  geom_histogram(binwidth = 200, position = "fill", alpha = 0.7) +
  scale_fill_manual(values = c("Yes" = "forestgreen", "No" = "gold2")) +
  theme_minimal()
```

## `forested` and `tree_no_tree` {.smaller}

```{r}
ggplot(forested_train, aes(x = tree_no_tree, fill = forested)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("Yes" = "forestgreen", "No" = "gold2")) +
  theme_minimal()
```

## `forested` and `lat` / `lon` {.smaller}

```{r}
ggplot(forested_train, aes(x = lon, y = lat, color = forested)) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = c("Yes" = "forestgreen", "No" = "gold2")) +
  theme_minimal()
```

# Terminology

## Recap: False negative and positive

-   **False negative rate** is the proportion of actual positives that were classified as negatives.

-   **False positive rate** is the proportion of actual negatives that were classified as positives.

## Recap: Sensitivity

**Sensitivity** is the proportion of actual positives that were correctly classified as positive.

-   Also known as **true positive rate** and **recall**

-   Sensitivity = 1 − False negative rate

-   Useful when false negatives are more "expensive" than false positives

## Recap: Specificity

**Specificity** is the proportion of actual negatives that were correctly classified as negative

-   Also known as **true negative rate**

-   Specificity = 1 − False positive rate

## ROC curve

The **receiver operating characteristic (ROC) curve** allows to assess the model performance across a range of thresholds.

![](images/roc-curve.png)

## ROC curve {.smaller}

::: {.columns}
::: {.column width=70%}
::: task
Which corner of the plot indicates the best model performance?
:::
:::
::: {.column}
:::
:::

![](images/roc-curve-annotated.png)

# Next steps

## Next steps

::: incremental
-   Fit models on training data

-   Make predictions on testing data

-   Evaluate predictions on testing data:

    -   Linear models: R-squared, adjusted R-squared, RMSE (root mean squared error), etc.
    -   Logistic models: False negative and positive rates, AUC (area under the curve), etc.

-   Make decisions based on model predictive performance, validity across various testing/training splits (aka "cross validation"), explainability
:::