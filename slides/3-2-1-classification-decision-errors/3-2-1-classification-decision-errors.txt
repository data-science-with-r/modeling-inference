>> In this video we'll discuyss classification and decision errors -- what can go wrong in classification and how we quantify the performance of models with regards to decisions we can make with them using a practical example of spam email detection.

>> Remember this promotional email that claimed you've been selected for an exclusive reward? We want to build a model, a spam filter, that classifies this email as a spam or not based on some of its features like word and character counts or even other more relevant features like word choices.

>> There are four things that can happen once we make a decision or classification -- 

> We might label an email that really is spam as spam, which would be a true positive.
> Or we might label an email that really is not spam as not spam, which would be a true negative. No errors so far. But you know sometimes spam filters into your inbox or real emails accidentally go to your spam box.
> An email labelled as spam when it's not is a false positive, what we call a Type 1 error.
> An email labelled as not spam when it is actually spam is a false negative, what we call a Type 2 error.
We'd prefer to never make either of these errors, but as you can imagine, that's not possible, or at least not likely.

> The false negative rate of a spam filter is the probability that an email that is spam is labelled as not spam. In other words -- number of false negatives divided by the sum of the number of true positives and false negatives.

> The false positive rate of a spam filter is the probability that an email that is not spam is labelled as spam. In other words -- number of false positives divided by the sum of the number of true negatives and false positives.

>> Two other terms of importance are sensitivity and specificity.

> Sensitivity is the complement of the false negative rate.

> Specificity is the complement of the false positive rate.

> If you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?

We want sensitivity and specificity to be high, but as you can imagine, as one goes up the other goes down. So next, we'll learn about how to evaluate model performance in light of these!
