---
title: "Main and interaction effects"
format: revealjs
---

```{r}
#| label: setup
#| include: false
ggplot2::theme_set(ggplot2::theme_gray(base_size = 16))
```

## Packages

- **DAAG** for data
- **tidyverse** for data wrangling and visualization
- **tidymodels** for modeling

```{r}
#| label: load-packages
#| warning: false
library(DAAG)
library(tidyverse)
library(tidymodels)
```

## Data: Book weight and volume

The `allbacks` data frame gives measurements on the volume and weights of 15 books, some of which are paperback and some of which are hardback

::: {.columns}
::: {.column width=40%}
- `volume` - cubic centimetres

- `area` - square centimetres

- `weight` - grams

- `cover` - `hb` or `pb`
:::
::: {.column width=60%}
```{r }
#| echo: false
as_tibble(allbacks)
```
:::
::: 

::: aside
These books are from the bookshelf of J. H. Maindonald at Australian National University.
:::

## Two possible explanations

::: {.columns}
::: {.column width=70%}
::: task
Suppose we want to predict weights of books from their volume and cover type (hardback vs. paperback). Do these visualizations suggest that a model that doesn't allow for the rate of change in weight to vary by cover type (parallel lines) is a better fit or a model that does allow (non-parallel lines)?
:::

```{r}
#| label: book-main-int
#| echo: false
#| fig-asp: 0.618
#| fig-width: 6
#| layout-ncol: 2
allbacks_main_fit <- linear_reg() |>
  fit(weight ~ volume + cover, data = allbacks)
allbacks_main_fit_aug <- augment(allbacks_main_fit, new_data = allbacks)

allbacks_int_fit <- linear_reg() |>
  fit(weight ~ volume + cover + volume*cover, data = allbacks)
allbacks_int_fit_aug <- augment(allbacks_int_fit, new_data = allbacks)

ggplot(
  allbacks_main_fit_aug, 
  aes(x = volume, color = cover)
  ) +
  geom_point(aes(y = weight, shape = cover), alpha = 0.7, size = 2) +
  geom_line(aes(y = .pred), linewidth = 1) +
  labs(
    title = "Main effects, parallel slopes", 
    subtitle = "weight-hat = volume + cover"
  ) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  theme(
    legend.position = "inside",
    legend.position.inside = c(0.1, 0.7)
  )

ggplot(
  allbacks_int_fit_aug,
  aes(x = volume, color = cover)
) +
  geom_point(aes(y = weight, shape = cover), alpha = 0.7, size = 2) +
  geom_line(aes(y = .pred), linewidth = 1) +
  labs(
    title = "Interaction effects, not parallel slopes", 
    subtitle = "weight-hat = volume + cover + volume * cover"
  ) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  guides(color = "none", shape = "none")
```

:::
:::

## In pursuit of Occam's razor

::: incremental
- Occam's Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.

- Model selection follows this principle.

- We only want to add another predictor to the model if the addition of that variable brings something valuable in terms of predictive power to the model.

- In other words, we prefer the simplest best model, i.e. **parsimonious** model.
:::

## In pursuit of Occam's razor

::: {.columns}
::: {.column width=70%}
::: task
Visually, which of the two models is preferable under Occam's razor?
:::

```{r}
#| ref.label: "book-main-int"
#| echo: false
#| fig-asp: 0.618
#| fig-width: 6
#| fig-ncol: 2
```

:::
::: 

## Model 1: `volume + cover` {.smaller}

**Main effects**: Rate of change for weight as volume increases is the same for hardback and paperback books.

```{r}
#| code-line-numbers: "|1|2|4"
allbacks_main_fit <- linear_reg() |>
  fit(weight ~ volume + cover, data = allbacks)

tidy(allbacks_main_fit)
```

. . .

```{r}
glance(allbacks_main_fit)
```

## Model 2: `volume + cover + volume*cover` {.smaller}

**Interaction effects**: Rate of change for weight as volume increases is different for hardback and paperback books.

```{r}
#| code-line-numbers: "|1|2|4"
allbacks_int_fit <- linear_reg() |>
  fit(weight ~ volume + cover + volume*cover, data = allbacks)

tidy(allbacks_int_fit)
```

 . . .

```{r}
glance(allbacks_main_fit)
```

## R's got your back!

When you add an interaction effect to a model, R will always add the main effects of those variables too, even if you leave them out of your model formula:

```{r}
#| code-line-numbers: "|2"
linear_reg() |>
  fit(weight ~ volume*cover, data = allbacks) |>
  tidy()
```

## Choosing between models {.smaller}

The model with the interaction effects has more predictors, and remember that when comparing models with different numbers of predictors, we use adjusted $R^2$ for model selection:

::: columns
::: {.column width=35%}

$R^2$:

```{r}
glance(allbacks_main_fit)$r.squared
glance(allbacks_int_fit)$r.squared
```
:::
::: {.column width=35%}

Adjusted $R^2$:

```{r}
glance(allbacks_main_fit)$adj.r.squared
glance(allbacks_int_fit)$adj.r.squared
```
:::
:::

. . .

- $R^2$ is higher for the model with the interaction effect.

- Adjusted $R^2$ is **not** higher for the model with the interaction effect, therefore we do **not** need the interaction effect, the main effects model is good enough!
