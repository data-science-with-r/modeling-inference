---
title: "Linear regression with multiple predictors"
format: revealjs
---

```{r}
#| label: setup
#| include: false
ggplot2::theme_set(ggplot2::theme_gray(base_size = 16))
```

## Packages

- **DAAG** for data
- **tidyverse** for data wrangling and visualization
- **tidymodels** for modeling

```{r}
#| label: load-packages
#| warning: false
library(DAAG)
library(tidyverse)
library(tidymodels)
```

## Data: Book weight and volume

The `allbacks` data frame gives measurements on the volume and weight of 15 books, some of which are paperback and some of which are hardback

::: {.columns}
::: {.column width=40%}
[- `volume` - cubic centimetres]{.fragment}

[- `area` - square centimetres]{.fragment}

[- `weight` - grams]{.fragment}

[- `cover` - `hb` or `pb`]{.fragment}
:::
::: {.column width=60%}
```{r }
#| echo: false
as_tibble(allbacks)
```
:::
::: 

::: aside
These books are from the bookshelf of J. H. Maindonald at Australian National University.
:::

## Book weight vs. volume

::: {.columns}
::: {.column width=65%}
```{r}
#| code-line-numbers: "|1|2|4"
allbacks_1_fit <- linear_reg() |>
  fit(weight ~ volume, data = allbacks)

tidy(allbacks_1_fit)
```
:::
::: {.column width=35%}
```{r}
#| echo: false
#| fig-width: 5
ggplot(allbacks, aes(x = volume, y = weight)) +
  geom_point(alpha = 0.7, size = 3)
```
:::
:::

## Book weight vs. volume and cover

::: {.columns}
::: {.column width=65%}
```{r}
#| code-line-numbers: "|2"
allbacks_2_fit <- linear_reg() |>
  fit(weight ~ volume + cover, data = allbacks)

tidy(allbacks_2_fit)
```
:::
::: {.column width=35%}
```{r}
#| echo: false
#| fig-width: 5
#| fig-asp: 0.8
ggplot(allbacks, aes(x = volume, y = weight, color = cover, shape = cover)) +
  geom_point(alpha = 0.7, size = 3) +
  theme(legend.position = "bottom") +
  scale_color_manual(values = c("#E48957", "#071381"))
```
:::
:::

## Interpretation of estimates

```{r}
tidy(allbacks_2_fit)
```

. . .

::: incremental
- **Slope - volume:** *Keeping cover constant*, for each additional cubic centimetre books are larger in volume, the model predicts the weight to be higher, on average, by 0.718 grams.

- **Slope - cover:** *Keeping volume constant*, the model predicts that paperback books weigh, on average, by 184 grams less than hardback books.

- **Intercept:** The model predicts that hardback books with 0 volume are expected to weigh 198 grams, on average. (Doesn't make sense in context.)
:::

## $R^2$

$R^2$ is the percentage of variability in the outcome explained by the regression model.

. . .

- Model 1: `weight ~ volume`

```{r}
glance(allbacks_1_fit)
```

. . .

- Model 2: `weight ~ volume + cover`

```{r}
glance(allbacks_2_fit)
```

. . .

- $R^2$ increases when **any** predictor is added to the model.

## Adjusted $R^2$

Adjusted $R^2$ adds a penalty to $R^2$ for additional predictors in the model, and is therefore a (more) objective measure for comparing models with different numbers of predictors.

. . .

- Model 1: `weight ~ volume`

```{r}
glance(allbacks_1_fit)$adj.r.squared
```

. . .

- Model 2: `weight ~ volume + cover`

```{r}
glance(allbacks_2_fit)$adj.r.squared
```

. . .

- Adjusted $R^2$ is higher for the model with `volume` and `cover` as predictors, and it is therefore the preferable model for predicting `weight`.

## Model 1 - visualized

```{r}
glance(allbacks_1_fit) |>
  select(r.squared, adj.r.squared)
```

```{r}
#| warning: false
#| echo: false
ggplot(allbacks, aes(x = volume, y = weight)) +
  geom_point(alpha = 0.7, size = 3) +
  geom_smooth(method = "lm", se = FALSE)
```

## Model 2 - visualized

```{r}
glance(allbacks_2_fit) |>
  select(r.squared, adj.r.squared)
```

```{r}
#| warning: false
#| echo: false
allbacks_2_aug <- augment(allbacks_2_fit, new_data = allbacks)

ggplot(allbacks_2_aug, aes(x = volume)) +
  geom_point(aes(y = weight, color = cover, shape = cover)) +
  geom_line(aes(y = .pred, color = cover)) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  theme(
    legend.position = "inside",
    legend.position.inside = c(0.15, 0.75)
  )
```

## Takeaways

::: incremental
- When interpreting slope coefficients for multiple regression models we need to state that one predictor is kept constant while the other increases.

- Adjusted R-squared is useful when comparing models with different numbers of predictors - it helps you balance model complexity with explanatory power.
:::
