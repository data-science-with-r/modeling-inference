[
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html",
    "title": "Modeling fish (Complete)",
    "section": "",
    "text": "Practice modeling using the fish dataset on two common fish species in fish market sales.\n\nWe will use the tidyverse package for data wrangling and visualization and the tidymodels package for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nThese data come from Kaggle and is commonly used in machine learning examples.\n\nfish &lt;- read_csv(\"https://data-science-with-r.github.io/data/fish.csv\")\n\nThe data dictionary is below:\n\n\nvariable\ndescription\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\nLet’s take a look at the data.\n\nfish\n\n# A tibble: 55 × 7\n   species weight length_vertical length_diagonal length_cross height width\n   &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bream      242            23.2            25.4         30     11.5  4.02\n 2 Bream      290            24              26.3         31.2   12.5  4.31\n 3 Bream      340            23.9            26.5         31.1   12.4  4.70\n 4 Bream      363            26.3            29           33.5   12.7  4.46\n 5 Bream      430            26.5            29           34     12.4  5.13\n 6 Bream      450            26.8            29.7         34.7   13.6  4.93\n 7 Bream      500            26.8            29.7         34.5   14.2  5.28\n 8 Bream      390            27.6            30           35     12.7  4.69\n 9 Bream      450            27.6            30           35.1   14.0  4.84\n10 Bream      500            28.5            30.7         36.2   14.2  4.96\n# ℹ 45 more rows"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#goal",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#goal",
    "title": "Modeling fish (Complete)",
    "section": "",
    "text": "Practice modeling using the fish dataset on two common fish species in fish market sales."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#packages",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#packages",
    "title": "Modeling fish (Complete)",
    "section": "",
    "text": "We will use the tidyverse package for data wrangling and visualization and the tidymodels package for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#data",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#data",
    "title": "Modeling fish (Complete)",
    "section": "",
    "text": "These data come from Kaggle and is commonly used in machine learning examples.\n\nfish &lt;- read_csv(\"https://data-science-with-r.github.io/data/fish.csv\")\n\nThe data dictionary is below:\n\n\nvariable\ndescription\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\nLet’s take a look at the data.\n\nfish\n\n# A tibble: 55 × 7\n   species weight length_vertical length_diagonal length_cross height width\n   &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bream      242            23.2            25.4         30     11.5  4.02\n 2 Bream      290            24              26.3         31.2   12.5  4.31\n 3 Bream      340            23.9            26.5         31.1   12.4  4.70\n 4 Bream      363            26.3            29           33.5   12.7  4.46\n 5 Bream      430            26.5            29           34     12.4  5.13\n 6 Bream      450            26.8            29.7         34.7   13.6  4.93\n 7 Bream      500            26.8            29.7         34.5   14.2  5.28\n 8 Bream      390            27.6            30           35     12.7  4.69\n 9 Bream      450            27.6            30           35.1   14.0  4.84\n10 Bream      500            28.5            30.7         36.2   14.2  4.96\n# ℹ 45 more rows"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#visualizing-the-model",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#visualizing-the-model",
    "title": "Modeling fish (Complete)",
    "section": "Visualizing the model",
    "text": "Visualizing the model\nWe’re going to investigate the relationship between the weights and heights of fish, predicting weight from height.\n\nCreate an appropriate plot to investigate this relationship. Add appropriate labels to the plot.\n\n\nggplot(fish, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n\n\n\n\n\n\n\nIf you were to draw a a straight line to best represent the relationship between the heights and weights of fish, where would it go? Why?\n\nStart from the bottom and go up. Identify the first and last point and draw a line through most the others.\n\nNow, let R draw the line for you! Hint: Use geom_smooth().\n\n\nggplot(fish, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Weights vs. lengths of fish\",\n    x = \"Head-to-tail lentgh (cm)\",\n    y = \"Weight of fish (grams)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhat types of questions can this plot help answer?\n\nIs there a relationship between fish heights and weights of fish?\n\nWe can use this line to make predictions. Predict what you think the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm. Which prediction is considered extrapolation?\n\nAt 10 cm, we estimate a weight of 375 grams. At 15 cm, we estimate a weight of 600 grams At 20 cm, we estimate a weight of 975 grams. 20 cm would be considered extrapolation.\n\nWhat is a residual?\n\nDifference between predicted and observed."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#model-fitting",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#model-fitting",
    "title": "Modeling fish (Complete)",
    "section": "Model fitting",
    "text": "Model fitting\n\nFit a model to predict fish weights from their heights.\n\n\nfish_hw_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ height, data = fish)\n\nfish_hw_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = weight ~ height, data = data)\n\nCoefficients:\n(Intercept)       height  \n    -288.42        60.92  \n\n\n\nPredict what the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm using this model.\n\n\nx &lt;- c(10, 15, 20)\n-288 + 60.92 * x\n\n[1] 321.2 625.8 930.4\n\n\n\nCalculate predicted weights for all fish in the data and visualize the residuals under this model.\n\n\nfish_hw_aug &lt;- augment(fish_hw_fit, new_data = fish)\n\nggplot(fish_hw_aug, aes(x = height, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +  \n  geom_segment(aes(xend = height, yend = .pred), color = \"gray\") +  \n  geom_point(aes(y = .pred), shape = \"circle open\") + \n  theme_minimal() +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    subtitle = \"Residuals\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#model-summary",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#model-summary",
    "title": "Modeling fish (Complete)",
    "section": "Model summary",
    "text": "Model summary\n\n\nDemo: Display the model summary including estimates for the slope and intercept along with measurements of uncertainty around them. Show how you can extract these values from the model output.\n\n\nfish_hw_tidy &lt;- tidy(fish_hw_fit)\nfish_hw_tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -288.      34.0      -8.49 1.83e-11\n2 height          60.9      2.64     23.1  2.40e-29\n\n\n\n\nDemo: Write out your model using mathematical notation.\n\n\\(\\widehat{weight} = -288 + 60.9 \\times height\\)"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#correlation",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#correlation",
    "title": "Modeling fish (Complete)",
    "section": "Correlation",
    "text": "Correlation\nWe can also assess correlation between two quantitative variables.\n\nWhat is correlation? What are values correlation can take?\n\nStrength and direction of a linear relationship. It’s bounded by -1 and 1.\n\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\nWhat is the correlation between heights and weights of fish?\n\n\nfish |&gt;\n  summarize(r = cor(height, weight))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.954"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#adding-a-third-variable",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#adding-a-third-variable",
    "title": "Modeling fish (Complete)",
    "section": "Adding a third variable",
    "text": "Adding a third variable\n\nDoes the relationship between heights and weights of fish change if we take into consideration species? Plot two separate straight lines for the Bream and Roach species.\n\n\nggplot(fish, aes(x = height, y = weight, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#fitting-other-models",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish-complete.html#fitting-other-models",
    "title": "Modeling fish (Complete)",
    "section": "Fitting other models",
    "text": "Fitting other models\n\nWe can fit more models than just a straight line. Change the following code below to read method = \"loess\". What is different from the plot created before?\n\n\nggplot(fish, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#packages",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#packages",
    "title": "Linear regression with a numerical predictor",
    "section": "Packages",
    "text": "Packages\n\n\nfivethirtyeight for data\n\ntidyverse for data wrangling and visualization\n\ntidymodels for modeling\n\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-prep",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-prep",
    "title": "Linear regression with a numerical predictor",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\n\nRename the dataset as movie_scores\n\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-overview",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-overview",
    "title": "Linear regression with a numerical predictor",
    "section": "Data overview",
    "text": "Data overview\n\nmovie_scores |&gt;\n  select(critics, audience)\n\n# A tibble: 146 × 2\n   critics audience\n     &lt;int&gt;    &lt;int&gt;\n 1      74       86\n 2      85       80\n 3      80       90\n 4      18       84\n 5      14       28\n 6      63       62\n 7      42       53\n 8      86       64\n 9      99       82\n10      89       87\n# ℹ 136 more rows"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-visualization",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#data-visualization",
    "title": "Linear regression with a numerical predictor",
    "section": "Data visualization",
    "text": "Data visualization"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#regression-model-1",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#regression-model-1",
    "title": "Linear regression with a numerical predictor",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the outcome, \\(Y\\), and the predictor, \\(X\\).\n\\[\n\\begin{aligned}\nY &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#regression-model-2",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#regression-model-2",
    "title": "Linear regression with a numerical predictor",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned}\nY &= \\color{#325b74}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{#325b74}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{#325b74}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#simple-linear-regression",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#simple-linear-regression",
    "title": "Linear regression with a numerical predictor",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nUse simple linear regression to model the relationship between a numerical outcome (\\(Y\\)) and a single numerical predictor (\\(X\\)): \\[\\Large{Y = \\beta_0 + \\beta_1 X + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(\\epsilon\\): Error (residual)"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#simple-linear-regression-1",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#simple-linear-regression-1",
    "title": "Linear regression with a numerical predictor",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\\[\n\\Large{\\hat{Y} = b_0 + b_1 X}\n\\]\n\n\n\\(b_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\n\n\\(b_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\n\nNo error term!"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#choosing-values-for-b_1-and-b_0",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#choosing-values-for-b_1-and-b_0",
    "title": "Linear regression with a numerical predictor",
    "section": "Choosing values for \\(b_1\\) and \\(b_0\\)\n",
    "text": "Choosing values for \\(b_1\\) and \\(b_0\\)"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#residuals",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#residuals",
    "title": "Linear regression with a numerical predictor",
    "section": "Residuals",
    "text": "Residuals\n\n\n\n\n\n\n\n\n\\[\n\\text{residual} = \\text{observed} - \\text{predicted} = y - \\hat{y}\n\\]"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#least-squares-line",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#least-squares-line",
    "title": "Linear regression with a numerical predictor",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[\ne_i = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\n\\]\n\nThe sum of squared residuals is\n\n\\[\ne^2_1 + e^2_2 + \\dots + e^2_n\n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#fitting-a-model",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#fitting-a-model",
    "title": "Linear regression with a numerical predictor",
    "section": "Fitting a model",
    "text": "Fitting a model\n\nmovies_fit &lt;- linear_reg() |&gt;\n  fit(audience ~ critics, data = movie_scores)\n\ntidy(movies_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-the-slope",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-the-slope",
    "title": "Linear regression with a numerical predictor",
    "section": "Interpreting the slope",
    "text": "Interpreting the slope\n\nThe slope of the model for predicting audience score from critics score is 0.519. Which of the following is the best interpretation of this value?\n\n\nFor every one point increase in the critics score, the audience score goes up by 0.519 points, on average.\nFor every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\nFor every one point increase in the critics score, the audience score goes up by 0.519 points.\nFor every one point increase in the audience score, the critics score goes up by 0.519 points, on average."
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-slope-intercept",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#interpreting-slope-intercept",
    "title": "Linear regression with a numerical predictor",
    "section": "Interpreting slope & intercept",
    "text": "Interpreting slope & intercept\n\\[\n\\widehat{\\text{audience}} = 32.3 + 0.519 \\times \\text{critics}\n\\]\n\n\nSlope: For every one point increase in the critics score, we expect the audience score to be higher by 0.519 points, on average.\n\nIntercept: If the critics score is 0 points, we expect the audience score to be 32.3 points."
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#is-the-intercept-meaningful",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#is-the-intercept-meaningful",
    "title": "Linear regression with a numerical predictor",
    "section": "Is the intercept meaningful?",
    "text": "Is the intercept meaningful?\n✅ The intercept is meaningful in context of the data if\n\nthe predictor can feasibly take values equal to or near zero or\nthe predictor has values near zero in the observed data\n\n\n🛑 Otherwise, it might not be meaningful!"
  },
  {
    "objectID": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#least-squares-regression",
    "href": "slides/1-2-1-linear-regression-numerical-predictor/1-2-1-linear-model-numerical-predictor.html#least-squares-regression",
    "title": "Linear regression with a numerical predictor",
    "section": "Least squares regression",
    "text": "Least squares regression\nThe least squares regression line minimizes the sum of squares residuals. It has the following properties:\n\nGoes through the center of mass point (the coordinates corresponding to average \\(X\\) and average \\(Y\\)): \\(b_0 = \\bar{Y} - b_1~\\bar{X}\\)\nSlope of the line has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_Y}{s_X}\\)\nSum of the residuals is zero: \\(\\sum_{i = 1}^n \\epsilon_i = 0\\)\nResiduals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#packages",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#packages",
    "title": "Linear regression with a categorical predictor",
    "section": "Packages",
    "text": "Packages\n\n\npalmerpenguins for data\n\ntidyverse for data wrangling and visualization\n\ntidymodels for modeling\n\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#data-penguins",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#data-penguins",
    "title": "Linear regression with a categorical predictor",
    "section": "Data: penguins\n",
    "text": "Data: penguins\n\nWe’ll work with the penguins dataset from the palmerpenguins package, which contains information on\n\npenguins\n\n# A tibble: 344 × 7\n   species island    bill_length_mm bill_depth_mm\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7\n 2 Adelie  Torgersen           39.5          17.4\n 3 Adelie  Torgersen           40.3          18  \n 4 Adelie  Torgersen           NA            NA  \n 5 Adelie  Torgersen           36.7          19.3\n 6 Adelie  Torgersen           39.3          20.6\n 7 Adelie  Torgersen           38.9          17.8\n 8 Adelie  Torgersen           39.2          19.6\n 9 Adelie  Torgersen           34.1          18.1\n10 Adelie  Torgersen           42            20.2\n# ℹ 334 more rows\n# ℹ 3 more variables: flipper_length_mm &lt;int&gt;,\n#   body_mass_g &lt;int&gt;, sex &lt;fct&gt;"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#variables",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#variables",
    "title": "Linear regression with a categorical predictor",
    "section": "Variables",
    "text": "Variables\n\nA researcher wants to study the relationship between body weights of penguins based on the island they were recorded on. How are the variables involved in this analysis different?\n\n\n\nOutcome: body weight (numerical)\nPredictor: island (categorical)"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#body-weight-vs.-island",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#body-weight-vs.-island",
    "title": "Linear regression with a categorical predictor",
    "section": "Body weight vs. island",
    "text": "Body weight vs. island\n\nDetermine whether each of the following plot types would be an appropriate choice for visualizing the relationship between body weight and island of penguins.\n\n\nScatterplot ❌\nBox plot ✅\nViolin plot ✅\nDensity plot ✅\nBar plot ❌\nStacked bar plot ❌"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#body-weight-vs.-island-1",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#body-weight-vs.-island-1",
    "title": "Linear regression with a categorical predictor",
    "section": "Body weight vs. island",
    "text": "Body weight vs. island\n\nggplot(\n  penguins, \n  aes(x = body_mass_g, y = island, color = island)\n  ) +\n  geom_boxplot(show.legend = FALSE)\n\nWarning: Removed 2 rows containing non-finite outside the scale\nrange (`stat_boxplot()`)."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#fitting-the-model",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#fitting-the-model",
    "title": "Linear regression with a categorical predictor",
    "section": "Fitting the model",
    "text": "Fitting the model\nLet’s fit a model to predict body weight from island:\n\n\nFit:\n\n\nbm_island_fit &lt;- linear_reg() |&gt;\n  fit(body_mass_g ~ island, data = penguins)\n\n\n\n\nTidy:\n\n\ntidy(bm_island_fit)\n\n# A tibble: 3 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        4716.      48.5      97.3 8.93e-250\n2 islandDream       -1003.      74.2     -13.5 1.42e- 33\n3 islandTorgersen   -1010.     100.      -10.1 4.66e- 21"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#inspecting-the-model-output",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#inspecting-the-model-output",
    "title": "Linear regression with a categorical predictor",
    "section": "Inspecting the model output",
    "text": "Inspecting the model output\n\nWhy is Biscoe not on the output?\n\n\n\n# A tibble: 3 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        4716.      48.5      97.3 8.93e-250\n2 islandDream       -1003.      74.2     -13.5 1.42e- 33\n3 islandTorgersen   -1010.     100.      -10.1 4.66e- 21\n\n\n\n\nWhen fitting a model with a categorical predictor, the levels of the categorical predictor are encoded to dummy variables, except for one of the levels, the baseline level.\nIn this case Biscoe is the is the baseline level.\nEach slope coefficient describes the predicted difference between heights in that particular school compared to the baseline level."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#dummy-variables",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#dummy-variables",
    "title": "Linear regression with a categorical predictor",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nisland\n\nDummy variable\n\n\n\nDream\nTorgersen\n\n\n\n\nBiscoe\n0\n0\n\n\nDream\n1\n0\n\n\ntorgersen\n0\n1\n\n\n\n\n\n\n\nFor a categorical predictor with \\(k\\) levels, we only need \\(k - 1\\) dummy variables to describe all of its levels:\n\n\nDream = 1 and Torgersen = 0, the penguin is from Dream island.\n\nDream = 0 and Torgersen = 1, the penguin is from Torgersen island.\n\nDream = 0 and Torgersen = 0, the penguin is from Biscoe island, we don’t need a third dummy variable to identify these penguins."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#dummy-coding",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#dummy-coding",
    "title": "Linear regression with a categorical predictor",
    "section": "Dummy coding",
    "text": "Dummy coding\n\n\n\n\n\n\nNote\n\n\nYou do not need to do anything (i.e., write code) to do the “dummy coding”, R does this under the hood for you when you have a predictor that is categorical (a character or a factor)."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#interpreting-the-model-output",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#interpreting-the-model-output",
    "title": "Linear regression with a categorical predictor",
    "section": "Interpreting the model output",
    "text": "Interpreting the model output\n\\[\n\\widehat{body~mass} = 4716 - 1003 \\times islandDream - 1010 \\times islandTorgersen\n\\]\n\nIntercept: Penguins from Biscoe island are expected to weigh, on average, 4,716 grams.\nSlope - islandDream: Penguins from Dream island are expected to weigh, on average, 1,003 grams less than those from Biscoe island.\nSlope - islandTorgersen: Penguins from Torgersen island are expected to weigh, on average, 1,010 grams less than those from Biscoe island."
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#predicting-based-on-the-model",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#predicting-based-on-the-model",
    "title": "Linear regression with a categorical predictor",
    "section": "Predicting based on the model",
    "text": "Predicting based on the model\n\\[\n\\widehat{body~mass} = 4716 - 1003 \\times islandDream - 1010 \\times islandTorgersen\n\\]\n\n\nBiscoe: \\(\\widehat{body~mass} = 4716 - 1003 \\times 0 - 1010 \\times 0 = 4716\\)\n\n\n\n\n\nDream: \\(\\widehat{body~mass} = 4716 - 1003 \\times 1 - 1010 \\times 0 = 3713\\)\n\n\n\n\n\nTorgersen: \\(\\widehat{body~mass} = 4716 - 1003 \\times 0 - 1010 \\times 1 = 3706\\)"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#predicting-based-on-the-model---again",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#predicting-based-on-the-model---again",
    "title": "Linear regression with a categorical predictor",
    "section": "Predicting based on the model - again",
    "text": "Predicting based on the model - again\n\nthree_penguins &lt;- tibble(\n  island = c(\"Biscoe\", \"Dream\", \"Torgersen\")\n  )\n\naugment(bm_island_fit, new_data = three_penguins)\n\n# A tibble: 3 × 2\n  .pred island   \n  &lt;dbl&gt; &lt;chr&gt;    \n1 4716. Biscoe   \n2 3713. Dream    \n3 3706. Torgersen"
  },
  {
    "objectID": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#models-with-categorical-predictors",
    "href": "slides/1-2-2-linear-regression-categorical-predictor/1-2-2-linear-model-categorical-predictor.html#models-with-categorical-predictors",
    "title": "Linear regression with a categorical predictor",
    "section": "Models with categorical predictors",
    "text": "Models with categorical predictors\n\nWhen the categorical predictor has many levels, they’re encoded to dummy variables.\nThe first level of the categorical variable is the baseline level. In a model with one categorical predictor, the intercept is the predicted value of the outcome for the baseline level (x = 0).\nEach slope coefficient describes the difference between the predicted value of the outcome for that level of the categorical variable compared to the baseline level."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#packages",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#packages",
    "title": "Linear model with a categorical predictor",
    "section": "Packages",
    "text": "Packages\n\n\npalmerpenguins for data\n\ntidyverse for data wrangling and visualization\n\ntidymodels for modeling\n\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#data-penguins",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#data-penguins",
    "title": "Linear model with a categorical predictor",
    "section": "Data: penguins\n",
    "text": "Data: penguins\n\nWe’ll work with the penguins dataset from the palmerpenguins package, which contains information on\n\npenguins\n\n# A tibble: 344 × 7\n   species island    bill_length_mm bill_depth_mm\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7\n 2 Adelie  Torgersen           39.5          17.4\n 3 Adelie  Torgersen           40.3          18  \n 4 Adelie  Torgersen           NA            NA  \n 5 Adelie  Torgersen           36.7          19.3\n 6 Adelie  Torgersen           39.3          20.6\n 7 Adelie  Torgersen           38.9          17.8\n 8 Adelie  Torgersen           39.2          19.6\n 9 Adelie  Torgersen           34.1          18.1\n10 Adelie  Torgersen           42            20.2\n# ℹ 334 more rows\n# ℹ 3 more variables: flipper_length_mm &lt;int&gt;,\n#   body_mass_g &lt;int&gt;, sex &lt;fct&gt;"
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#variables",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#variables",
    "title": "Linear model with a categorical predictor",
    "section": "Variables",
    "text": "Variables\n\nA researcher wants to study the relationship between body weights of penguins based on the island they were recorded on. How are the variables involved in this analysis different?\n\n\n\nOutcome: body weight (numerical)\nPredictor: island (categorical)"
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#body-weight-vs.-island",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#body-weight-vs.-island",
    "title": "Linear model with a categorical predictor",
    "section": "Body weight vs. island",
    "text": "Body weight vs. island\n\nDetermine whether each of the following plot types would be an appropriate choice for visualizing the relationship between body weight and island of penguins.\n\n\nScatterplot ❌\nBox plot ✅\nViolin plot ✅\nDensity plot ✅\nBar plot ❌\nStacked bar plot ❌"
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#body-weight-vs.-island-1",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#body-weight-vs.-island-1",
    "title": "Linear model with a categorical predictor",
    "section": "Body weight vs. island",
    "text": "Body weight vs. island\n\nggplot(\n  penguins, \n  aes(x = body_mass_g, y = island, color = island)\n  ) +\n  geom_boxplot(show.legend = FALSE)\n\nWarning: Removed 2 rows containing non-finite outside the scale\nrange (`stat_boxplot()`)."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#fitting-the-model",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#fitting-the-model",
    "title": "Linear model with a categorical predictor",
    "section": "Fitting the model",
    "text": "Fitting the model\nLet’s fit a model to predict body weight from island:\n\n\nFit:\n\n\nbm_island_fit &lt;- linear_reg() |&gt;\n  fit(body_mass_g ~ island, data = penguins)\n\n\n\n\nTidy:\n\n\ntidy(bm_island_fit)\n\n# A tibble: 3 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        4716.      48.5      97.3 8.93e-250\n2 islandDream       -1003.      74.2     -13.5 1.42e- 33\n3 islandTorgersen   -1010.     100.      -10.1 4.66e- 21"
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#inspecting-the-model-output",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#inspecting-the-model-output",
    "title": "Linear model with a categorical predictor",
    "section": "Inspecting the model output",
    "text": "Inspecting the model output\n\nWhy is Biscoe not on the output?\n\n\n\n# A tibble: 3 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        4716.      48.5      97.3 8.93e-250\n2 islandDream       -1003.      74.2     -13.5 1.42e- 33\n3 islandTorgersen   -1010.     100.      -10.1 4.66e- 21\n\n\n\n\nWhen fitting a model with a categorical predictor, the levels of the categorical predictor are encoded to dummy variables, except for one of the levels, the baseline level.\nIn this case Biscoe is the is the baseline level.\nEach slope coefficient describes the predicted difference between heights in that particular school compared to the baseline level."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#dummy-variables",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#dummy-variables",
    "title": "Linear model with a categorical predictor",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nisland\n\nDummy variable\n\n\n\nDream\nTorgersen\n\n\n\n\nBiscoe\n0\n0\n\n\nDream\n1\n0\n\n\ntorgersen\n0\n1\n\n\n\n\n\n\n\nFor a categorical predictor with \\(k\\) levels, we only need \\(k - 1\\) dummy variables to describe all of its levels:\n\n\nDream = 1 and Torgersen = 0, the penguin is from Dream island.\n\nDream = 0 and Torgersen = 1, the penguin is from Torgersen island.\n\nDream = 0 and Torgersen = 0, the penguin is from Biscoe island, we don’t need a third dummy variable to identify these penguins."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#dummy-coding",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#dummy-coding",
    "title": "Linear model with a categorical predictor",
    "section": "Dummy coding",
    "text": "Dummy coding\n\n\n\n\n\n\nNote\n\n\nYou do not need to do anything (i.e., write code) to do the “dummy coding”, R does this under the hood for you when you have a predictor that is categorical (a character or a factor)."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#interpreting-the-model-output",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#interpreting-the-model-output",
    "title": "Linear model with a categorical predictor",
    "section": "Interpreting the model output",
    "text": "Interpreting the model output\n\\[\n\\widehat{body~mass} = 4716 - 1003 \\times islandDream - 1010 \\times islandTorgersen\n\\]\n\nIntercept: Penguins from Biscoe island are expected to weigh, on average, 4,716 grams.\nSlope - islandDream: Penguins from Dream island are expected to weigh, on average, 1,003 grams less than those from Biscoe island.\nSlope - islandTorgersen: Penguins from Torgersen island are expected to weigh, on average, 1,010 grams less than those from Biscoe island."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#predicting-based-on-the-model",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#predicting-based-on-the-model",
    "title": "Linear model with a categorical predictor",
    "section": "Predicting based on the model",
    "text": "Predicting based on the model\n\\[\n\\widehat{body~mass} = 4716 - 1003 \\times islandDream - 1010 \\times islandTorgersen\n\\]\n\n\nBiscoe: \\(\\widehat{body~mass} = 4716 - 1003 \\times 0 - 1010 \\times 0 = 4716\\)\n\n\n\n\n\nDream: \\(\\widehat{body~mass} = 4716 - 1003 \\times 1 - 1010 \\times 0 = 3713\\)\n\n\n\n\n\nTorgersen: \\(\\widehat{body~mass} = 4716 - 1003 \\times 0 - 1010 \\times 1 = 3706\\)"
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#predicting-based-on-the-model---again",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#predicting-based-on-the-model---again",
    "title": "Linear model with a categorical predictor",
    "section": "Predicting based on the model - again",
    "text": "Predicting based on the model - again\n\nthree_penguins &lt;- tibble(\n  island = c(\"Biscoe\", \"Dream\", \"Torgersen\")\n  )\n\naugment(bm_island_fit, new_data = three_penguins)\n\n# A tibble: 3 × 2\n  .pred island   \n  &lt;dbl&gt; &lt;chr&gt;    \n1 4716. Biscoe   \n2 3713. Dream    \n3 3706. Torgersen"
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#models-with-categorical-predictors",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#models-with-categorical-predictors",
    "title": "Linear model with a categorical predictor",
    "section": "Models with categorical predictors",
    "text": "Models with categorical predictors\n\nWhen the categorical predictor has many levels, they’re encoded to dummy variables.\nThe first level of the categorical variable is the baseline level. In a model with one categorical predictor, the intercept is the predicted value of the outcome for the baseline level (x = 0).\nEach slope coefficient describes the difference between the predicted value of the outcome for that level of the categorical variable compared to the baseline level."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n Modeling and inference",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nLanguage of models\n\n\n\n\n\nLinear regression with a numerical predictor\n\n\n\n\n\nLinear regression with a categorical predictor\n\n\n\n\n\nOutliers in linear regression\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "\n Modeling and inference",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nLanguage of models\n\n\n\n\n\nLinear regression with a numerical predictor\n\n\n\n\n\nLinear regression with a categorical predictor\n\n\n\n\n\nOutliers in linear regression\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#code-alongs",
    "href": "index.html#code-alongs",
    "title": "\n Modeling and inference",
    "section": "Code alongs",
    "text": "Code alongs\n\n\n\n\nTitle\n\n\n\n\n\n\nModeling fish (Complete)\n\n\n\n\n\nModeling fish\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "\n Modeling and inference",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nTitle\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#transform---visualize",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#transform---visualize",
    "title": "Welcome",
    "section": "Transform <-> visualize",
    "text": "Transform &lt;-&gt; visualize"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#import---tidy",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#import---tidy",
    "title": "Welcome",
    "section": "Import <-> tidy",
    "text": "Import &lt;-&gt; tidy"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#data-science-ethics",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#data-science-ethics",
    "title": "Welcome",
    "section": "Data science ethics",
    "text": "Data science ethics\n\nMisrepresentation\nData privacy\nAlgorithmic bias"
  },
  {
    "objectID": "slides/1-1-1-welcome/1-1-1-welcome.html#modeling-and-inference",
    "href": "slides/1-1-1-welcome/1-1-1-welcome.html#modeling-and-inference",
    "title": "Welcome",
    "section": "Modeling and inference",
    "text": "Modeling and inference\n\n\n\n\n\n\n\n\n\nFitting, interpreting, selecting, and evaluating models\nPrediction, classification, and assessing accuracy\nMaking inferences and quantifying uncertainty"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling",
    "title": "Language of models",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but remember there are many many other types of models too!)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-cars",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-cars",
    "title": "Language of models",
    "section": "Modeling cars",
    "text": "Modeling cars\n\n\nWhat is the relationship between cars’ weights and their mileage?\nWhat is your best guess for a car’s MPG that weighs 3,500 pounds?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars",
    "title": "Language of models",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nDescribe: What is the relationship between cars’ weights and their mileage?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modelling-cars-1",
    "title": "Language of models",
    "section": "Modelling cars",
    "text": "Modelling cars\n\nPredict: What is your best guess for a car’s MPG that weighs 3,500 pounds?"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-1",
    "title": "Language of models",
    "section": "Modeling",
    "text": "Modeling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we will focus on linear models (but there are many many other types of models too!)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-vocabulary",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#modeling-vocabulary",
    "title": "Language of models",
    "section": "Modeling vocabulary",
    "text": "Modeling vocabulary\n\nOutcome: Variable whose behavior or variation you are trying to understand, on the y-axis (aka response variable, dependent variable)\nPredictor(s): Other variable(s) that you want to use to explain the variation in the outcome, on the x-axis (aka explanatory variable(s), independent variable(s))\nModel function: The regression line for predicting the outcome variable from the predictor variable(s), comprised generally of an intercept and a slope for each predictor\nPredicted value: Output of the model function, which gives the typical (expected) value of the outcome conditioning on the predictor\n\nResiduals: A measure of how far each case’s observed value is from its predicted value (based on a particular model)\n\nResidual = Observed value - Predicted value\nTells how far above/below the expected value each case is"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#predictor",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#predictor",
    "title": "Language of models",
    "section": "Predictor",
    "text": "Predictor\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#outcome",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#outcome",
    "title": "Language of models",
    "section": "Outcome",
    "text": "Outcome\n\n\n\n\n\n\n\n\nmpg\nwt\n\n\n\n21\n2.62\n\n\n21\n2.875\n\n\n22.8\n2.32\n\n\n21.4\n3.215\n\n\n18.7\n3.44\n\n\n18.1\n3.46\n\n\n...\n..."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line",
    "title": "Language of models",
    "section": "Regression line",
    "text": "Regression line"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-slope",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-slope",
    "title": "Language of models",
    "section": "Regression line: slope",
    "text": "Regression line: slope"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-intercept",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#regression-line-intercept",
    "title": "Language of models",
    "section": "Regression line: intercept",
    "text": "Regression line: intercept"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation",
    "title": "Language of models",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#correlation-1",
    "title": "Language of models",
    "section": "Correlation",
    "text": "Correlation\n\nRanges between -1 and 1.\nSame sign as the slope."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#visualizing-the-model",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#visualizing-the-model",
    "title": "Language of models",
    "section": "Visualizing the model",
    "text": "Visualizing the model\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#residuals",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#residuals",
    "title": "Language of models",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#extending-regression-lines",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#extending-regression-lines",
    "title": "Language of models",
    "section": "Extending regression lines",
    "text": "Extending regression lines"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#models---upsides-and-downsides",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#models---upsides-and-downsides",
    "title": "Language of models",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modeling over simple visual inspection of data.\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#variation-around-the-model",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#variation-around-the-model",
    "title": "Language of models",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\nStatistics is the explanation of variation in the context of what remains unexplained.\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#how-do-we-use-models",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#how-do-we-use-models",
    "title": "Language of models",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nPredict / classify: Plug in the value(s) of predictor(s) to the model to obtain the predicted value of the outcome\nDescribe: Quantify the relationship between predictor(s) and outcome with slopes"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#self-driving-cars",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#self-driving-cars",
    "title": "Language of models",
    "section": "Self driving cars",
    "text": "Self driving cars"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#semi-or-garage",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#semi-or-garage",
    "title": "Language of models",
    "section": "Semi or garage?",
    "text": "Semi or garage?\n\ni love how Tesla thinks the wall in my garage is a semi. 😅\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#semi-or-garage-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#semi-or-garage-1",
    "title": "Language of models",
    "section": "Semi or garage?",
    "text": "Semi or garage?\n\nNew owner here. Just parked in my garage. Tesla thinks I crashed onto a semi.\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#car-or-trash",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#car-or-trash",
    "title": "Language of models",
    "section": "Car or trash?",
    "text": "Car or trash?\n\nTesla calls Mercedes trash\n\n\n\n\n\n\n\nSource: Reddit"
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp",
    "title": "Language of models",
    "section": "Leisure, commute, physical activity and BP",
    "text": "Leisure, commute, physical activity and BP\n\nRelation Between Leisure Time, Commuting, and Occupational Physical Activity With Blood Pressure in 125,402 Adults: The Lifelines Cohort\nByambasukh, Oyuntugs, Harold Snieder, and Eva Corpeleijn. “Relation between leisure time, commuting, and occupational physical activity with blood pressure in 125 402 adults: the lifelines cohort.” Journal of the American Heart Association 9.4 (2020): e014313."
  },
  {
    "objectID": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp-1",
    "href": "slides/1-1-2-language-of-models/1-1-2-language-of-models.html#leisure-commute-physical-activity-and-bp-1",
    "title": "Language of models",
    "section": "Leisure, commute, physical activity and BP",
    "text": "Leisure, commute, physical activity and BP\nBackground: Whether all domains of daily‐life moderate‐to‐vigorous physical activity (MVPA) are associated with lower blood pressure (BP) and how this association depends on age and body mass index remains unclear.\nMethods and Results: In the population‐based Lifelines cohort (N=125,402), MVPA was assessed by the Short Questionnaire to Assess Health‐Enhancing Physical Activity, a validated questionnaire in different domains such as commuting, leisure‐time, and occupational PA. BP was assessed using the last 3 of 10 measurements after 10 minutes’ rest in the supine position. Hypertension was defined as systolic BP ≥140 mm Hg and/or diastolic BP ≥90 mm Hg and/or use of antihypertensives. In regression analysis, higher commuting and leisure‐time but not occupational MVPA related to lower BP and lower hypertension risk. Commuting‐and‐leisure‐time MVPA was associated with BP in a dose‐dependent manner. β Coefficients (95% CI) from linear regression analyses were −1.64 (−2.03 to −1.24), −2.29 (−2.68 to −1.90), and finally −2.90 (−3.29 to −2.50) mm Hg systolic BP for the low, middle, and highest tertile of MVPA compared with “No MVPA” as the reference group after adjusting for age, sex, education, smoking and alcohol use. Further adjustment for body mass index attenuated the associations by 30% to 50%, but more MVPA remained significantly associated with lower BP and lower risk of hypertension. This association was age dependent. β Coefficients (95% CI) for the highest tertiles of commuting‐and‐leisure‐time MVPA were −1.67 (−2.20 to −1.15), −3.39 (−3.94 to −2.82) and −4.64 (−6.15 to −3.14) mm Hg systolic BP in adults &lt;40, 40 to 60, and &gt;60 years, respectively.\nConclusions: Higher commuting and leisure‐time but not occupational MVPA were significantly associated with lower BP and lower hypertension risk at all ages, but these associations were stronger in older adults."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html",
    "title": "Modeling fish",
    "section": "",
    "text": "Practice modeling using the fish dataset on two common fish species in fish market sales.\n\nWe will use the tidyverse package for data wrangling and visualization and the tidymodels package for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nThese data come from Kaggle and is commonly used in machine learning examples.\n\nfish &lt;- read_csv(\"https://data-science-with-r.github.io/data/fish.csv\")\n\nThe data dictionary is below:\n\n\nvariable\ndescription\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\nLet’s take a look at the data.\n\nfish\n\n# A tibble: 55 × 7\n   species weight length_vertical length_diagonal length_cross height width\n   &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bream      242            23.2            25.4         30     11.5  4.02\n 2 Bream      290            24              26.3         31.2   12.5  4.31\n 3 Bream      340            23.9            26.5         31.1   12.4  4.70\n 4 Bream      363            26.3            29           33.5   12.7  4.46\n 5 Bream      430            26.5            29           34     12.4  5.13\n 6 Bream      450            26.8            29.7         34.7   13.6  4.93\n 7 Bream      500            26.8            29.7         34.5   14.2  5.28\n 8 Bream      390            27.6            30           35     12.7  4.69\n 9 Bream      450            27.6            30           35.1   14.0  4.84\n10 Bream      500            28.5            30.7         36.2   14.2  4.96\n# ℹ 45 more rows"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#goal",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#goal",
    "title": "Modeling fish",
    "section": "",
    "text": "Practice modeling using the fish dataset on two common fish species in fish market sales."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#packages",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#packages",
    "title": "Modeling fish",
    "section": "",
    "text": "We will use the tidyverse package for data wrangling and visualization and the tidymodels package for modeling.\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#data",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#data",
    "title": "Modeling fish",
    "section": "",
    "text": "These data come from Kaggle and is commonly used in machine learning examples.\n\nfish &lt;- read_csv(\"https://data-science-with-r.github.io/data/fish.csv\")\n\nThe data dictionary is below:\n\n\nvariable\ndescription\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\nLet’s take a look at the data.\n\nfish\n\n# A tibble: 55 × 7\n   species weight length_vertical length_diagonal length_cross height width\n   &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 Bream      242            23.2            25.4         30     11.5  4.02\n 2 Bream      290            24              26.3         31.2   12.5  4.31\n 3 Bream      340            23.9            26.5         31.1   12.4  4.70\n 4 Bream      363            26.3            29           33.5   12.7  4.46\n 5 Bream      430            26.5            29           34     12.4  5.13\n 6 Bream      450            26.8            29.7         34.7   13.6  4.93\n 7 Bream      500            26.8            29.7         34.5   14.2  5.28\n 8 Bream      390            27.6            30           35     12.7  4.69\n 9 Bream      450            27.6            30           35.1   14.0  4.84\n10 Bream      500            28.5            30.7         36.2   14.2  4.96\n# ℹ 45 more rows"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#visualizing-the-model",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#visualizing-the-model",
    "title": "Modeling fish",
    "section": "Visualizing the model",
    "text": "Visualizing the model\nWe’re going to investigate the relationship between the weights and heights of fish, predicting weight from height.\n\nCreate an appropriate plot to investigate this relationship. Add appropriate labels to the plot.\n\n\n# add code here\n\n\nIf you were to draw a a straight line to best represent the relationship between the heights and weights of fish, where would it go? Why?\n\nAdd response here.\n\nNow, let R draw the line for you! Hint: Use geom_smooth().\n\n\n# add code here\n\n\nWhat types of questions can this plot help answer?\n\nAdd response here.\n\nWe can use this line to make predictions. Predict what you think the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm. Which prediction is considered extrapolation?\n\nAdd response here.\n\nWhat is a residual?\n\nAdd response here."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#model-fitting",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#model-fitting",
    "title": "Modeling fish",
    "section": "Model fitting",
    "text": "Model fitting\n\nFit a model to predict fish weights from their heights.\n\n\n# add code here\n\n\nPredict what the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm using this model.\n\n\n# add code here\n\n\nCalculate predicted weights for all fish in the data and visualize the residuals under this model.\n\n\n# add code here"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#model-summary",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#model-summary",
    "title": "Modeling fish",
    "section": "Model summary",
    "text": "Model summary\n\n\nDemo: Display the model summary including estimates for the slope and intercept along with measurements of uncertainty around them. Show how you can extract these values from the model output.\n\n\n# add code here\n\n\n\nDemo: Write out your model using mathematical notation.\n\nAdd response here."
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#correlation",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#correlation",
    "title": "Modeling fish",
    "section": "Correlation",
    "text": "Correlation\nWe can also assess correlation between two quantitative variables.\n\nWhat is correlation? What are values correlation can take?\n\nAdd response here.\n\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\nWhat is the correlation between heights and weights of fish?\n\n\n# add code here"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#adding-a-third-variable",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#adding-a-third-variable",
    "title": "Modeling fish",
    "section": "Adding a third variable",
    "text": "Adding a third variable\n\nDoes the relationship between heights and weights of fish change if we take into consideration species? Plot two separate straight lines for the Bream and Roach species.\n\n\n# add code here"
  },
  {
    "objectID": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#fitting-other-models",
    "href": "code-alongs/1-2-modeling-fish/1-2-modeling-fish.html#fitting-other-models",
    "title": "Modeling fish",
    "section": "Fitting other models",
    "text": "Fitting other models\n\nWe can fit more models than just a straight line. Change the following code below to read method = \"loess\". What is different from the plot created before?\n\n\n# add code here"
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#outliers-in-regression",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#outliers-in-regression",
    "title": "Outliers in linear regression",
    "section": "Outliers in regression",
    "text": "Outliers in regression\n\nOutliers are observations that fall far from the main cloud of points.\n\nThey can be outlying in:\n\nthe \\(x\\) direction,\nthe \\(y\\) direction, or\nboth.\n\n\nHowever, being outlying in a univariate sense does not always mean being outlying from the bivariate model.\nPoints that are in-line with the bivariate model usually do not influence the least squares line, even if they are extreme in \\(x\\), \\(y\\), or both."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#outliers-and-influence",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#outliers-and-influence",
    "title": "Outliers in linear regression",
    "section": "Outliers and influence",
    "text": "Outliers and influence\n\nd1 &lt;- simulated_scatter |&gt;\n  filter(group == 24) |&gt;\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\nd2 &lt;- simulated_scatter |&gt;\n  filter(group == 25) |&gt;\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\nd3 &lt;- simulated_scatter |&gt;\n  filter(group == 26) |&gt;\n  mutate(outlier = if_else(y == max(y), TRUE, FALSE))\n\nm1_aug &lt;- augment(lm(y ~ x, data = d1)) |&gt;\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\n\nm2_aug &lt;- augment(lm(y ~ x, data = d2)) |&gt;\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\n\nm3_aug &lt;- augment(lm(y ~ x, data = d3)) |&gt;\n  mutate(outlier = if_else(y == max(y), TRUE, FALSE))\n\np_1 &lt;- ggplot(d1, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d1 |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"red\", \"full\"], stroke = 2\n  ) +\n  labs(title = \"A\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\nWarning: The `size` argument of `element_rect()` is deprecated as of\nggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\np_1_res &lt;- ggplot(m1_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m1_aug |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"red\", \"full\"], stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_2 &lt;- ggplot(d2, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d2 |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"green\", \"full\"], stroke = 2\n  ) +\n  labs(title = \"B\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\np_2_res &lt;- ggplot(m2_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m2_aug |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"green\", \"full\"], stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_3 &lt;- ggplot(d3, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d3 |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"pink\", \"full\"], stroke = 2\n  ) +\n  labs(title = \"C\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\np_3_res &lt;- ggplot(m3_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m3_aug |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"pink\", \"full\"], stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_1 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_2 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_3 +\n  p_1_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_2_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_3_res +\n  plot_layout(ncol = 3, heights = c(2, 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nA: One outlier in the \\(y\\) direction, also outlying in the bivariate model; slightly influences the regression line.\n\nB: One outlier on the right (outlying in \\(x\\) and \\(y\\), but not outlying in the bivariate model); close to the regression line and not influential.\n\nC: One point far from the cloud (outlying in \\(x\\), \\(y\\), and bivariate model); pulls the regression line upward, worsening fit for the main data."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#outliers-and-influence-1",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#outliers-and-influence-1",
    "title": "Outliers in linear regression",
    "section": "Outliers and influence",
    "text": "Outliers and influence\n\nd4 &lt;- simulated_scatter |&gt;\n  filter(group == 27)\n\nd5 &lt;- simulated_scatter |&gt; filter(group == 28)\nd6 &lt;- simulated_scatter |&gt; filter(group == 29)\n\nm4_aug &lt;- augment(lm(y ~ x, data = d4))\nm5_aug &lt;- augment(lm(y ~ x, data = d5))\nm6_aug &lt;- augment(lm(y ~ x, data = d6))\n\np_4 &lt;- ggplot(d4, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  labs(title = \"D\")\n\np_4_res &lt;- ggplot(m4_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  ylim(-4, 4)\n\np_5 &lt;- ggplot(d5, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  labs(title = \"E\")\n\np_5_res &lt;- ggplot(m5_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  ylim(-4, 4)\n\np_6 &lt;- ggplot(d6, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  labs(title = \"F\")\n\np_6_res &lt;- ggplot(m6_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  ylim(-4, 4)\n\np_4 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_5 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_6 +\n  p_4_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_5_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_6_res +\n  plot_layout(ncol = 3, heights = c(2, 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing missing values or values outside\nthe scale range (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nD: A secondary small cloud of four points (outlying in \\(x\\) and bivariate model); strongly influences the regression line, creating poor fit.\n\nE: Outlier far right (outlying in \\(x\\) and \\(y\\)); the regression line is largely controlled by this single point, imposing a trend where there is none.\n\nF: One outlier far away (outlying in \\(x\\) and \\(y\\)), but in-line with the model; has little influence."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#types-of-outliers",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#types-of-outliers",
    "title": "Outliers in linear regression",
    "section": "Types of outliers",
    "text": "Types of outliers\n\nOutliers: Points or groups of points that stand out from the rest of the data.\nLeverage points: Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with high leverage or leverage points.\n\nInfluential points: Outliers, generally high leverage points, that actually alter the slope or position of the regression line.\n\nWe say a point is influential if omitting it would substantially change the regression model."
  },
  {
    "objectID": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#practical-advice",
    "href": "slides/1-2-3-outliers-linear-regression/1-2-3-outliers-linear-regression.html#practical-advice",
    "title": "Outliers in linear regression",
    "section": "Practical advice",
    "text": "Practical advice\n\nTest your analysis with and without outliers.\nCompare and discuss the impact of outliers on model fit.\nPresent both models to stakeholders to choose the most reasonable interpretation.\n\n\n\n\n\n\n\n\nWarning\n\n\nRemoving outliers should only be done with strong justification – excluding interesting or extreme cases can lead to misleading models, poor predictive performance, and flawed conclusions."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-in-regression",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-in-regression",
    "title": "Outliers in linear regression",
    "section": "Outliers in regression",
    "text": "Outliers in regression\n\nOutliers are observations that fall far from the main cloud of points.\n\nThey can be outlying in:\n\nthe \\(x\\) direction,\nthe \\(y\\) direction, or\nboth.\n\n\nHowever, being outlying in a univariate sense does not always mean being outlying from the bivariate model.\nPoints that are in-line with the bivariate model usually do not influence the least squares line, even if they are extreme in \\(x\\), \\(y\\), or both."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-and-influence",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-and-influence",
    "title": "Outliers in linear regression",
    "section": "Outliers and influence",
    "text": "Outliers and influence\n\nd1 &lt;- simulated_scatter |&gt;\n  filter(group == 24) |&gt;\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\nd2 &lt;- simulated_scatter |&gt;\n  filter(group == 25) |&gt;\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\nd3 &lt;- simulated_scatter |&gt;\n  filter(group == 26) |&gt;\n  mutate(outlier = if_else(y == max(y), TRUE, FALSE))\n\nm1_aug &lt;- augment(lm(y ~ x, data = d1)) |&gt;\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\n\nm2_aug &lt;- augment(lm(y ~ x, data = d2)) |&gt;\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\n\nm3_aug &lt;- augment(lm(y ~ x, data = d3)) |&gt;\n  mutate(outlier = if_else(y == max(y), TRUE, FALSE))\n\np_1 &lt;- ggplot(d1, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d1 |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"red\", \"full\"], stroke = 2\n  ) +\n  labs(title = \"A\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\nWarning: The `size` argument of `element_rect()` is deprecated as of\nggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\np_1_res &lt;- ggplot(m1_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m1_aug |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"red\", \"full\"], stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_2 &lt;- ggplot(d2, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d2 |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"green\", \"full\"], stroke = 2\n  ) +\n  labs(title = \"B\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\np_2_res &lt;- ggplot(m2_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m2_aug |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"green\", \"full\"], stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_3 &lt;- ggplot(d3, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d3 |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"pink\", \"full\"], stroke = 2\n  ) +\n  labs(title = \"C\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\np_3_res &lt;- ggplot(m3_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m3_aug |&gt; filter(outlier),\n    size = 5, shape = \"circle open\",\n    color = IMSCOL[\"pink\", \"full\"], stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_1 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_2 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_3 +\n  p_1_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_2_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_3_res +\n  plot_layout(ncol = 3, heights = c(2, 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nA: One outlier in the \\(y\\) direction, also outlying in the bivariate model; slightly influences the regression line.\n\nB: One outlier on the right (outlying in \\(x\\) and \\(y\\), but not outlying in the bivariate model); close to the regression line and not influential.\n\nC: One point far from the cloud (outlying in \\(x\\), \\(y\\), and bivariate model); pulls the regression line upward, worsening fit for the main data."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-and-influence-1",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#outliers-and-influence-1",
    "title": "Outliers in linear regression",
    "section": "Outliers and influence",
    "text": "Outliers and influence\n\nd4 &lt;- simulated_scatter |&gt;\n  filter(group == 27)\n\nd5 &lt;- simulated_scatter |&gt; filter(group == 28)\nd6 &lt;- simulated_scatter |&gt; filter(group == 29)\n\nm4_aug &lt;- augment(lm(y ~ x, data = d4))\nm5_aug &lt;- augment(lm(y ~ x, data = d5))\nm6_aug &lt;- augment(lm(y ~ x, data = d6))\n\np_4 &lt;- ggplot(d4, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  labs(title = \"D\")\n\np_4_res &lt;- ggplot(m4_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  ylim(-4, 4)\n\np_5 &lt;- ggplot(d5, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  labs(title = \"E\")\n\np_5_res &lt;- ggplot(m5_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  ylim(-4, 4)\n\np_6 &lt;- ggplot(d6, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  labs(title = \"F\")\n\np_6_res &lt;- ggplot(m6_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, size = 1)\n    ) +\n  ylim(-4, 4)\n\np_4 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_5 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_6 +\n  p_4_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_5_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_6_res +\n  plot_layout(ncol = 3, heights = c(2, 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing missing values or values outside\nthe scale range (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nD: A secondary small cloud of four points (outlying in \\(x\\) and bivariate model); strongly influences the regression line, creating poor fit.\n\nE: Outlier far right (outlying in \\(x\\) and \\(y\\)); the regression line is largely controlled by this single point, imposing a trend where there is none.\n\nF: One outlier far away (outlying in \\(x\\) and \\(y\\)), but in-line with the model; has little influence."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#types-of-outliers",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#types-of-outliers",
    "title": "Outliers in linear regression",
    "section": "Types of outliers",
    "text": "Types of outliers\n\nOutliers: Points or groups of points that stand out from the rest of the data.\nLeverage points: Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with high leverage or leverage points.\n\nInfluential points: Outliers, generally high leverage points, that actually alter the slope or position of the regression line.\n\nWe say a point is influential if omitting it would substantially change the regression model."
  },
  {
    "objectID": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#practical-advice",
    "href": "slides/1-3-outliers-linear-regression/1-3-outliers-linear-regression.html#practical-advice",
    "title": "Outliers in linear regression",
    "section": "Practical advice",
    "text": "Practical advice\n\nTest your analysis with and without outliers.\nCompare and discuss the impact of outliers on model fit.\nPresent both models to stakeholders to choose the most reasonable interpretation.\n\n\n\n\n\n\n\n\nWarning\n\n\nRemoving outliers should only be done with strong justification – excluding interesting or extreme cases can lead to misleading models, poor predictive performance, and flawed conclusions."
  }
]