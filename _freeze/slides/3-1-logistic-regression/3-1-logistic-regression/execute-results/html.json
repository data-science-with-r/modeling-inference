{
  "hash": "c4081ddb3676fe6f64c3f41488972627",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic regression\"\nformat: revealjs\n---\n\n\n\n## Packages\n\n- **tidyverse** for data wrangling and visualization\n- **tidymodels** for modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggthemes)\nlibrary(openintro)\nlibrary(fivethirtyeight)\nlibrary(palmerpenguins)\n```\n:::\n\n\n# Regression so far\n\n## Recap: Simple linear regression {.smaller}\n\nNumerical outcome and one numerical predictor:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/movies-1.png){width=768}\n:::\n:::\n\n\n## Recap: Multiple linear regression {.smaller}\n\nNumerical outcome and one categorical predictor (two levels):\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/smoking-1.png){width=768}\n:::\n:::\n\n\n## Recap: multiple linear regression {.smaller}\n\nNumerical outcome, numerical and categorical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/penguins-1.png){width=768}\n:::\n:::\n\n\n# New scenario: *binary* outcome\n\n## A *binary* outcome {.smaller}\n\n$$\ny = \n\\begin{cases}\n1 & &&\\text{eg. Yes, Win, True, Heads, Success}\\\\\n0 & &&\\text{eg. No, Lose, False, Tails, Failure}.\n\\end{cases}\n$$\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/sample-df-S-curve-1.png){width=768}\n:::\n:::\n\n\n## Who cares?\n\nIf we can model the relationship between predictors ($x$) and a binary outcome ($y$), we can use the model to do a special kind of prediction called *classification*.\n\n## Example: Is the e-mail spam or not? {.smaller}\n\n::: {.columns}\n::: {.column width=\"20%\"}\n\n$\\mathbf{x}$: Word and character counts in an e-mail\n\n:::\n::: {.column width=\"50%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's spam}\\\\\n0 & \\text{it's legit}\n\\end{cases}\n$$\n\n:::\n:::\n\n::: {.columns}\n::: {.column width=70%}\n::: email\nSubject: Congratulations! You‚Äôve Been Selected for an Exclusive Reward üéÅ\n\nDear Customer,\n\nYou have been chosen as one of our preferred recipients to receive a special complimentary gift. This is our way of thanking you for your continued interest in our services.\n\nTo claim your reward, simply complete our short survey. Your participation takes only 60 seconds, and your prize will be shipped at no cost to you.\n\nClick here to start your survey and claim your reward\n[Claim Reward Link]\n\nThis exclusive offer is available for the next 48 hours only. Don‚Äôt miss your chance to enjoy this limited opportunity.\n\nWarm regards,  \nPromotions Team  \nExclusive Rewards Center\n:::\n:::\n::: {.column width=30%}\n:::\n:::\n\n::: aside\nSample spam email language generated by Chat GPT with the prompt \"Generate a fake ‚Äúpromotional‚Äù style spam email that doesn't contain any explicit words.\"\n:::\n\n## Example: Is it cancer or not? {.smaller}\n\n::: {.columns}\n::: {.column width=\"20%\"}\n\n$\\mathbf{x}$: features in a medical image\n\n:::\n::: {.column width=\"50%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's cancer}\\\\\n0 & \\text{it's healthy}\n\\end{cases}\n$$\n\n:::\n:::\n\n![](images/national-cancer-institute-BDKid0yJcAk-unsplash.png){fig-align=\"center\" width=600px}\n\n\n::: aside\nPhoto by <a href=\"https://unsplash.com/@nci?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">National Cancer Institute</a> on <a href=\"https://unsplash.com/photos/a-black-and-white-photo-of-various-mri-images-BDKid0yJcAk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>.\n:::\n\n## Example: Will they default? {.smaller}\n\n::: {.columns}\n::: {.column width=\"20%\"}\n\n$\\mathbf{x}$: financial and demographic info about a loan applicant\n\n:::\n::: {.column width=\"50%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{applicant is at risk of defaulting on loan}\\\\\n0 & \\text{applicant is safe}\n\\end{cases}\n$$\n\n:::\n:::\n\n![](images/credit-card-6401275_640.png){fig-align=\"center\" width=600px}\n\n::: aside\nPhoto by <a href=\"https://pixabay.com/illustrations/credit-card-credit-score-mastercard-6401275/\">PabitraKaity</a> on <a href=\"https://pixabay.com/illustrations/credit-card-credit-score-mastercard-6401275/\">Pixabay</a>.\n:::\n\n## Example: Will they reoffend? {.smaller}\n\n::: {.columns}\n::: {.column width=\"20%\"}\n\n$\\mathbf{x}$: info about a criminal suspect and their case\n\n:::\n::: {.column width=\"50%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{suspect is at risk of re-offending pre-trial}\\\\\n0 & \\text{suspect is safe}\n\\end{cases}\n$$\n\n:::\n:::\n\n![](images/machine-bias-petty-theft-1.png){fig-align=\"center\" width=400px} ![](images/machine-bias-drug-posession-1.png){fig-align=\"center\" width=400px}\n\n::: aside\n[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) on ProPublica.\n:::\n\n## How do we model this type of data?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/sample-df-data-1.png){width=768}\n:::\n:::\n\n\n## Straight line of best fit is a little silly\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/sample-df-line-1.png){width=768}\n:::\n:::\n\n\n## Instead: S-curve of best fit {.smaller}\n\nInstead of modeling $y$ directly, we model the probability that $y=1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/sample-df-S-curve-again-1.png){width=768}\n:::\n:::\n\n\n::: incremental\n-   \"Given new email, what's the probability that it's spam?\"\n-   \"Given new image, what's the probability that it's cancer?\"\n-   \"Given new loan application, what's the probability that applicant defaults?\"\n:::\n\n## Why don't we model y directly?\n\n-   **Recall regression with a numerical outcome**:\n\n    -   Our models do not output *guarantees* for $y$, they output predictions that describe behavior *on average*;\n\n. . .\n\n-   **Similar when modeling a binary outcome**:\n\n    -   Our models cannot directly guarantee that $y$ will be zero or one. The correct analog to \"on average\" for a 0/1 outcome is \"what's the probability?\"\n\n## So, what is this S-curve, anyway?\n\nIt's the **logistic function**:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}.\n$$\n\n. . .\n\nIf you set p = Prob(y = 1) and do some algebra, you get the simple linear model for the *log-odds*:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\nThis is called the *logistic regression* model.\n\n## Log-odds?\n\n::: incremental\n-   $p = Prob(y = 1)$ is a probability -- A number between 0 and 1\n\n-   $p / (1 - p)$ is the odds -- A number between 0 and $\\infty$\n\n-   The log odds $log(p / (1 - p))$ is a number between $-\\infty$ and $\\infty$, which is suitable for the linear model\n::: \n\n## Logistic regression\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x\n$$\n\n-   The *logit* function $log(p / (1-p))$ is an example of a **link function** that transforms the linear model to have an appropriate range\n\n-   This is an example of a **generalized linear model**\n\n## Estimation\n\n-   We estimate the parameters $\\beta_0$, $\\beta_1$, etc. using *maximum likelihood* (don't worry about it) to get the \"best fitting\" S-curve\n\n-   The fitted model is\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x\n$$\n\n# Classification\n\n## Step 1: Pick a threshold {.smaller}\n\nSelect a number $0 < p^* < 1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/step-1-1.png){width=672}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$\n\n## Step 2: Find the decision boundary {.smaller}\n\nSolve for the x-value that matches the threshold:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/step-2-1.png){width=672}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$\n\n## Step 3: Classify a new arrival {.smaller}\n\nA new data point is observed up with $x_{\\text{new}}$.\nWhich side of the boundary is it on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/step-3-1.png){width=672}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new observation\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new observation\n\n## Let's change the threshold {.smaller}\n\nA new data point is observed with $x_{\\text{new}}$.\nWhich side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/lower-threshold-1.png){width=672}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new observation\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new observation\n\n## Nothing special about one predictor... {.smaller}\n\nTwo numerical predictors and one binary outcome:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## \"Multiple\" logistic regression\n\nOn the probability scale:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}.\n$$\n\nFor the log-odds, a *multiple* linear regression:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m.\n$$\n\n## Decision boundary, again {.smaller}\n\nIt's linear!\nConsider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](3-1-logistic-regression_files/figure-revealjs/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$. Predict $\\widehat{y}=0$ for the new observation\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$. Predict $\\widehat{y}=1$ for the new observation\n",
    "supporting": [
      "3-1-logistic-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}