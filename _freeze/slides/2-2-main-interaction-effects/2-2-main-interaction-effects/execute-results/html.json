{
  "hash": "2af1e9af6b7f1b0db04042911acdb44e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear regression with multiple predictors\"\nformat: revealjs\n---\n\n\n\n## Packages\n\n- **DAAG** for data\n- **tidyverse** for data wrangling and visualization\n- **tidymodels** for modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DAAG)\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n\n## Data: Book weight and volume\n\nThe `allbacks` data frame gives measurements on the volume and weight of 15 books, some of which are paperback and some of which are hardback\n\n::: {.columns}\n::: {.column width=40%}\n- Volume - cubic centimetres\n- Area - square centimetres\n- Weight - grams\n:::\n::: {.column width=60%}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 4\n   volume  area weight cover\n    <dbl> <dbl>  <dbl> <fct>\n 1    885   382    800 hb   \n 2   1016   468    950 hb   \n 3   1125   387   1050 hb   \n 4    239   371    350 hb   \n 5    701   371    750 hb   \n 6    641   367    600 hb   \n 7   1228   396   1075 hb   \n 8    412     0    250 pb   \n 9    953     0    700 pb   \n10    929     0    650 pb   \n11   1492     0    975 pb   \n12    419     0    350 pb   \n13   1010     0    950 pb   \n14    595     0    425 pb   \n15   1034     0    725 pb   \n```\n\n\n:::\n:::\n\n:::\n::: \n\n::: aside\nThese books are from the bookshelf of J. H. Maindonald at Australian National University.\n:::\n\n## Book weight vs. volume\n\n::: {.columns}\n::: {.column width=65%}\n\n::: {.cell}\n\n```{.r .cell-code}\nallbacks_1_fit <- linear_reg() |>\n  fit(weight ~ volume, data = allbacks)\n\ntidy(allbacks_1_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)  108.      88.4         1.22 0.245     \n2 volume         0.709    0.0975      7.27 0.00000626\n```\n\n\n:::\n:::\n\n:::\n::: {.column width=35%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-2-main-interaction-effects_files/figure-html/unnamed-chunk-3-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n## Book weight vs. volume and cover\n\n::: {.columns}\n::: {.column width=65%}\n\n::: {.cell}\n\n```{.r .cell-code}\nallbacks_2_fit <- linear_reg() |>\n  fit(weight ~ volume + cover, data = allbacks)\n\ntidy(allbacks_2_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n```\n\n\n:::\n:::\n\n:::\n::: {.column width=35%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-2-main-interaction-effects_files/figure-html/unnamed-chunk-5-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n## Interpretation of estimates\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(allbacks_2_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n```\n\n\n:::\n:::\n\n\n. . .\n\n- **Slope - volume:** *All else held constant*, for each additional cubic centimetre books are larger in volume, we would expect the weight to be higher, on average, by 0.718 grams.\n\n. . .\n\n- **Slope - cover:** *All else held constant*, paperback books are weigh, on average, by 184 grams less than hardcover books.\n\n. . .\n\n- **Intercept:** Hardcover books with 0 volume are expected to weigh 198 grams, on average. (Doesn't make sense in context.)\n\n## Main vs. interaction effects {.smaller}\n\n::: {.columns}\n::: {.column width=45%}\n::: task\nSuppose we want to predict weight of books from their volume and cover type \n(hardback vs. paperback). Do you think a model with main effects or \ninteraction effects is more appropriate? Explain your reasoning.\n\n**Hint:** Main effects would mean rate at which weight changes as volume \nincreases would be the same for hardback and paperback books and interaction \neffects would mean the rate at which weight changes as volume \nincreases would be different for hardback and paperback books.\n:::\n:::\n::: {.column width=5%}\n:::\n::: {.column width=45%}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-2-main-interaction-effects_files/figure-html/book-main-int-1.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](2-2-main-interaction-effects_files/figure-html/book-main-int-2.png){width=576}\n:::\n:::\n\n\n:::\n:::\n\n\n## In pursuit of Occam's razor\n\n- Occam's Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\n\n. . .\n\n- Model selection follows this principle.\n\n. . .\n\n- We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\n\n. . .\n\n- In other words, we prefer the simplest best model, i.e. **parsimonious** model.\n\n## In pursuit of Occam's razor\n\n::: {.columns}\n::: {.column width=70%}\n::: task\nVisually, which of the two models is preferable under Occam's razor?\n:::\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](2-2-main-interaction-effects_files/figure-html/unnamed-chunk-7-1.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](2-2-main-interaction-effects_files/figure-html/unnamed-chunk-7-2.png){width=576}\n:::\n:::\n\n\n:::\n::: \n\n\n## R-squared\n\n- $R^2$ is the percentage of variability in the outcome explained by the regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(book_main_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9274776\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(book_int_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9297137\n```\n\n\n:::\n:::\n\n\n. . .\n\n- The model with interaction effect has a slightly higher $R^2$.\n\n. . .\n\n- However using $R^2$ for model selection in models with multiple explanatory variables is not a good idea as $R^2$ increases when **any** variable is added to the model.\n\n## Adjusted R-squared\n\n... a (more) objective measure for model selection\n\n- Adjusted $R^2$ doesn't increase if the new variable does not provide any new  informaton or is completely unrelated, as it applies a penalty for number of  variables included in the model.\n\n- This makes adjusted $R^2$ a preferable metric for model selection in multiple regression models.\n\n## Comparing models\n\n::: columns\n::: {.column width=45%}\n\n$R^2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(book_main_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9274776\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(book_int_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9297137\n```\n\n\n:::\n:::\n\n:::\n::: {.column width=45%}\n\nAdjusted $R^2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(book_main_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9153905\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(book_int_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9105447\n```\n\n\n:::\n:::\n\n:::\n:::\n\n. . .\n\n::: columns\n::: {.column width=70%}\n\n- $R^2$ is higher for the model with the interaction effect.\n\n- Adjusted $R^2$ is **not** higher for the model with the interaction effect.\n\n:::\n:::\n",
    "supporting": [
      "2-2-main-interaction-effects_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}