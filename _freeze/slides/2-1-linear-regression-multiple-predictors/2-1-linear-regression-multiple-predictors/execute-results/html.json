{
  "hash": "5076b8df32dc04b3521c71dc6f613797",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear regression with multiple predictors\"\nformat: revealjs\n---\n\n\n\n## Packages\n\n- **DAAG** for data\n- **tidyverse** for data wrangling and visualization\n- **tidymodels** for modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DAAG)\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n\n## Data: Book weight and volume\n\nThe `allbacks` data frame gives measurements on the volume and weight of 15 books, some of which are paperback and some of which are hardback\n\n::: {.columns}\n::: {.column width=40%}\n[- `volume` - cubic centimetres]{.fragment}\n\n[- `area` - square centimetres]{.fragment}\n\n[- `weight` - grams]{.fragment}\n\n[- `cover` - `hb` or `pb`]{.fragment}\n:::\n::: {.column width=60%}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 4\n   volume  area weight cover\n    <dbl> <dbl>  <dbl> <fct>\n 1    885   382    800 hb   \n 2   1016   468    950 hb   \n 3   1125   387   1050 hb   \n 4    239   371    350 hb   \n 5    701   371    750 hb   \n 6    641   367    600 hb   \n 7   1228   396   1075 hb   \n 8    412     0    250 pb   \n 9    953     0    700 pb   \n10    929     0    650 pb   \n11   1492     0    975 pb   \n12    419     0    350 pb   \n13   1010     0    950 pb   \n14    595     0    425 pb   \n15   1034     0    725 pb   \n```\n\n\n:::\n:::\n\n:::\n::: \n\n::: aside\nThese books are from the bookshelf of J. H. Maindonald at Australian National University.\n:::\n\n## Book weight vs. volume\n\n::: {.columns}\n::: {.column width=65%}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|2|4\"}\nallbacks_1_fit <- linear_reg() |>\n  fit(weight ~ volume, data = allbacks)\n\ntidy(allbacks_1_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)  108.      88.4         1.22 0.245     \n2 volume         0.709    0.0975      7.27 0.00000626\n```\n\n\n:::\n:::\n\n:::\n::: {.column width=35%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-1-linear-regression-multiple-predictors_files/figure-revealjs/unnamed-chunk-3-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n## Book weight vs. volume and cover\n\n::: {.columns}\n::: {.column width=65%}\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2\"}\nallbacks_2_fit <- linear_reg() |>\n  fit(weight ~ volume + cover, data = allbacks)\n\ntidy(allbacks_2_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n```\n\n\n:::\n:::\n\n:::\n::: {.column width=35%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-1-linear-regression-multiple-predictors_files/figure-revealjs/unnamed-chunk-5-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\n## Interpretation of estimates\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(allbacks_2_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n```\n\n\n:::\n:::\n\n\n. . .\n\n::: incremental\n- **Slope - volume:** *Keeping cover constant*, for each additional cubic centimetre books are larger in volume, the model predicts the weight to be higher, on average, by 0.718 grams.\n\n- **Slope - cover:** *Keeping volume constant*, the model predicts that paperback books weigh, on average, by 184 grams less than hardback books.\n\n- **Intercept:** The model predicts that hardback books with 0 volume are expected to weigh 198 grams, on average. (Doesn't make sense in context.)\n:::\n\n## $R^2$ {.smaller}\n\n$R^2$ is the percentage of variability in the outcome explained by the regression model.\n\n. . .\n\n- Model 1: `weight ~ volume`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(allbacks_1_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df\n      <dbl>         <dbl> <dbl>     <dbl>      <dbl> <dbl>\n1     0.803         0.787  124.      52.9 0.00000626     1\n# ℹ 6 more variables: logLik <dbl>, AIC <dbl>, BIC <dbl>,\n#   deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n. . .\n\n- Model 2: `weight ~ volume + cover`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(allbacks_2_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df\n      <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>\n1     0.927         0.915  78.2      76.7 0.000000145     2\n# ℹ 6 more variables: logLik <dbl>, AIC <dbl>, BIC <dbl>,\n#   deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n. . .\n\n- $R^2$ increases when **any** predictor is added to the model.\n\n## Adjusted $R^2$ {.smaller}\n\nAdjusted $R^2$ adds a penalty to $R^2$ for additional predictors in the model, and is therefore a (more) objective measure for comparing models with different numbers of predictors.\n\n. . .\n\n- Model 1: `weight ~ volume`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(allbacks_1_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7874526\n```\n\n\n:::\n:::\n\n\n. . .\n\n- Model 2: `weight ~ volume + cover`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(allbacks_2_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9153905\n```\n\n\n:::\n:::\n\n\n. . .\n\n- Adjusted $R^2$ is higher for the model with `volume` and `cover` as predictors, and it is therefore the preferable model for predicting `weight`.\n\n## Model 1 - visualized\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(allbacks_1_fit) |>\n  select(r.squared, adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      <dbl>         <dbl>\n1     0.803         0.787\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-1-linear-regression-multiple-predictors_files/figure-revealjs/unnamed-chunk-12-1.png){width=768}\n:::\n:::\n\n\n## Model 2 - visualized\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(allbacks_2_fit) |>\n  select(r.squared, adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      <dbl>         <dbl>\n1     0.927         0.915\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2-1-linear-regression-multiple-predictors_files/figure-revealjs/unnamed-chunk-14-1.png){width=768}\n:::\n:::\n\n\n## Takeaways\n\n::: incremental\n- When interpreting slope coefficients for multiple regression models we need to state that one predictor is kept constant while the other increases.\n\n- Adjusted R-squared is useful when comparing models with different numbers of predictors - it helps you balance model complexity with explanatory power.\n:::\n",
    "supporting": [
      "2-1-linear-regression-multiple-predictors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}